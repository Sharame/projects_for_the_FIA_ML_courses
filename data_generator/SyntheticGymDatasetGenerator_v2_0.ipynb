{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Gym Dataset Generator v2.0 - Set-Level (Experience-Aware)\n",
        "\n",
        "**Progetto**: Sistema multi-modulo ML per supporto decisionale training fisico  \n",
        "**Versione**: 2.0 (Experience-Aware Behavioral Patterns)  \n",
        "** testo in grassettoAutore**: Alessandro Ambrosio\n",
        "\n",
        "---\n",
        "\n",
        "## **CHANGELOG v1.0 → v2.0**\n",
        "\n",
        "### **Problemi Identificati v1.0**\n",
        "- Skip rate sempre 0.0 (no dropout pattern)\n",
        "- Consistency score sempre 1.0 (non informativo)\n",
        "- RPE variability uniforme tra livelli\n",
        "- Spike frequency random (non auto-regolazione)\n",
        "- Total sets range overlap eccessivo\n",
        "\n",
        "### **Modifiche v2.0 (Scientifically Grounded)**\n",
        "\n",
        "1. **Skip Rate Experience-Aware** (Sperandei et al. 2016)\n",
        "   - Beginner: 12-15% skip rate\n",
        "   - Intermediate: 6-9%\n",
        "   - Advanced: 4-6%\n",
        "\n",
        "2. **RPE Calibration** (Day et al. 2004, Helms et al. 2016)\n",
        "   - Beginner: std=1.5, bias +0.5 (sovrastimano)\n",
        "   - Advanced: std=0.6, bias -0.2 (sottostimano)\n",
        "\n",
        "3. **Spike Self-Regulation** (Gabbett 2016)\n",
        "   - Beginner: 18-22% settimane spike\n",
        "   - Advanced: 5-8% settimane spike\n",
        "\n",
        "4. **Training History Duration** (Rønnestad & Mujika 2014)\n",
        "   - Beginner: 3-9 mesi storia\n",
        "   - Advanced: 18-36 mesi storia\n",
        "\n",
        "5. **Consistency Post-Processing**\n",
        "   - Beginner: 65-80%\n",
        "   - Advanced: 85-95%\n",
        "\n",
        "---\n",
        "\n",
        "## **Bibliografia Chiave**\n",
        "\n",
        "- **Banister (1975)**: Fitness-Fatigue impulse-response model\n",
        "- **Gabbett (2016)**: ACWR spike detection (threshold 1.5-1.6)\n",
        "- **Day et al. (2004)**: Novizi sovrastimano RPE\n",
        "- **Helms et al. (2016)**: Esperti hanno RPE variability -60%\n",
        "- **Sperandei et al. (2016)**: Dropout rate novizi 2.5x vs esperti\n",
        "- **Rønnestad & Mujika (2014)**: Training history volume predice performance\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0brUgLknx8lC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 2 - Setup & Imports**"
      ],
      "metadata": {
        "id": "bmY6rZsFyLLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SETUP ENVIRONMENT\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "!pip -q install pandas numpy\n",
        "\n",
        "import os, json, math\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from datetime import date, timedelta, datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"[OK]OK] Libraries imported\")\n",
        "print(f\"[OK] NumPy version: {np.__version__}\")\n",
        "print(f\"[OK] Pandas version: {pd.__version__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_ldGUpGyQXR",
        "outputId": "6670e6b3-6899-466f-fba9-450e2a50127d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK]OK] Libraries imported\n",
            "[OK] NumPy version: 2.0.2\n",
            "[OK] Pandas version: 2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**LOAD EXISTING DATASET (skip generation)**"
      ],
      "metadata": {
        "id": "O2x0TDmp_LrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# LOAD EXISTING DATASET (skip generation)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "LOAD_EXISTING = False  # ← Set True per caricare, False per rigenerare\n",
        "\n",
        "if LOAD_EXISTING:\n",
        "    print(\"=\"*80)\n",
        "    print(\"LOADING EXISTING DATASET v2.0\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    DATADIR = Path('data/synth_set_level_v2')\n",
        "\n",
        "    # Check if files exist\n",
        "    required_files = [\n",
        "        'users.csv',\n",
        "        'workout_sets.csv',\n",
        "        'workouts.csv',\n",
        "        'workout_plan.csv',\n",
        "        'banister_daily.csv',\n",
        "        'banister_meta.csv',\n",
        "        'exercises.csv'\n",
        "    ]\n",
        "\n",
        "    missing = [f for f in required_files if not (DATADIR / f).exists()]\n",
        "\n",
        "    if missing:\n",
        "        print(f\"[!] Missing files: {missing}\")\n",
        "        print(\"-> Set LOAD_EXISTING = False and re-run generator\")\n",
        "        raise FileNotFoundError(\"Dataset files not found\")\n",
        "\n",
        "    # Load all CSVs\n",
        "    df_users = pd.read_csv(DATADIR / 'users.csv')\n",
        "    df_exercises = pd.read_csv(DATADIR / 'exercises.csv')\n",
        "    df_workouts = pd.read_csv(DATADIR / 'workouts.csv')\n",
        "    df_plan = pd.read_csv(DATADIR / 'workout_plan.csv')\n",
        "    df_sets = pd.read_csv(DATADIR / 'workout_sets.csv')\n",
        "    df_banister_daily = pd.read_csv(DATADIR / 'banister_daily.csv')\n",
        "    df_banister_meta = pd.read_csv(DATADIR / 'banister_meta.csv')\n",
        "\n",
        "    # Parse dates\n",
        "    df_workouts['date'] = pd.to_datetime(df_workouts['date'])\n",
        "    df_sets['date'] = pd.to_datetime(df_sets['date'])\n",
        "    df_banister_daily['date'] = pd.to_datetime(df_banister_daily['date'])\n",
        "\n",
        "    print(f\"\\n[OK] Dataset loaded from {DATADIR}\")\n",
        "    print(f\"[OK] Users: {len(df_users):,}\")\n",
        "    print(f\"[OK] Workouts: {len(df_workouts):,}\")\n",
        "    print(f\"[OK] Sets: {len(df_sets):,}\")\n",
        "\n",
        "    # Skip generation cells\n",
        "    print(\"\\n[!] SKIP CELLE 13-16 (già eseguiti in precedenza)\")\n",
        "    print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "k4aE4JlK_L64"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 3 - Configuration v2.0**"
      ],
      "metadata": {
        "id": "pNtFoxenyN8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# CONFIGURATION v2.0 (Experience-Aware Parameters)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "@dataclass\n",
        "class CFG:\n",
        "    # Random seed\n",
        "    seed: int = 99999\n",
        "    out_dir: str = \"data/synth_set_level_v2\"\n",
        "\n",
        "    # User generation\n",
        "    n_users: int = 1500\n",
        "\n",
        "    # Date ranges (PER-USER, stratificato per experience in generazione)\n",
        "    today: date = date.today()\n",
        "    end_date_max_days_ahead: int = 730\n",
        "\n",
        "    # Training schedule\n",
        "    weekly_freq_mu: float = 3.5\n",
        "    weekly_freq_sd: float = 1.0\n",
        "    weekly_freq_min: int = 1\n",
        "    weekly_freq_max: int = 6\n",
        "    weekday_jitter_probs = [0.15, 0.70, 0.15]  # [-1, 0, +1]\n",
        "\n",
        "    # Quantization\n",
        "    load_step: float = 0.25\n",
        "    rpe_step: float = 0.5\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # SKIP MODEL v2.0 (Experience-Aware)\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    # Baseline skip probability per livello (Sperandei et al. 2016)\n",
        "    skip_p0_by_level: dict = None\n",
        "\n",
        "    # Fatigue sensitivity per livello\n",
        "    skip_fatigue_sensitivity: dict = None\n",
        "\n",
        "    skip_fatigue_cap: float = 1.2\n",
        "    skip_noise_sd: float = 0.10\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # RPE CALIBRATION v2.0 (Day et al. 2004, Helms et al. 2016)\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    rpe_params_by_level: dict = None\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # SPIKE ACWR v2.0 (Gabbett 2016 + Experience Auto-regulation)\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    spike_enabled: bool = True\n",
        "\n",
        "    # Probabilità spike/deload per livello\n",
        "    spike_deload_probs: dict = None\n",
        "\n",
        "    spike_acwr_min: float = 1.35\n",
        "    spike_acwr_max: float = 1.85\n",
        "    deload_acwr_min: float = 0.65\n",
        "    deload_acwr_max: float = 0.85\n",
        "    normal_acwr_mean: float = 1.05\n",
        "    normal_acwr_std: float = 0.10\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # TRAINING HISTORY DURATION (Rønnestad & Mujika 2014)\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    duration_days_by_level: dict = None\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # CONSISTENCY TARGET RANGES\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    consistency_targets: dict = None\n",
        "\n",
        "    # Injury\n",
        "    injury_lambda: float = 0.002\n",
        "    injury_days_min: int = 7\n",
        "    injury_days_max: int = 28\n",
        "\n",
        "    # Missingness\n",
        "    p_missing_rpe: float = 0.02\n",
        "    p_missing_load: float = 0.01\n",
        "    p_missing_feedback: float = 0.02\n",
        "\n",
        "    # Banister\n",
        "    tau_F_mean: float = 45.0\n",
        "    tau_F_sd: float = 8.0\n",
        "    tau_D_mean: float = 7.0\n",
        "    tau_D_sd: float = 2.0\n",
        "    beta_F: float = 0.010\n",
        "    beta_D: float = 0.015\n",
        "\n",
        "    # Progressive overload\n",
        "    overload_base_rate: dict = None\n",
        "    overload_I0: float = 1800.0\n",
        "    overload_quality_fatigue: float = 0.6\n",
        "    transfer_same_muscle: float = 0.35\n",
        "    transfer_same_split: float = 0.12\n",
        "\n",
        "    # User profiles\n",
        "    user_profiles: dict = None\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "# INITIALIZE CONFIG v2.0\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Skip Model Parameters (Experience-Aware)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "cfg.skip_p0_by_level = {\n",
        "    'Beginner': 0.13,       # Target: 12-15% skip rate\n",
        "    'Intermediate': 0.075,  # Target: 6-9% skip rate\n",
        "    'Advanced': 0.05        # Target: 4-6% skip rate\n",
        "}\n",
        "\n",
        "cfg.skip_fatigue_sensitivity = {\n",
        "    'Beginner': 0.35,       # Più sensibili a fatica\n",
        "    'Intermediate': 0.25,\n",
        "    'Advanced': 0.15        # Meno sensibili\n",
        "}\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# RPE Calibration Parameters (Experience-Aware)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "cfg.rpe_params_by_level = {\n",
        "    'Beginner': {\n",
        "        'noise_std': 1.5,    # Alta variabilità\n",
        "        'bias': 0.5          # Sovrastimano sforzo\n",
        "    },\n",
        "    'Intermediate': {\n",
        "        'noise_std': 1.0,\n",
        "        'bias': 0.0\n",
        "    },\n",
        "    'Advanced': {\n",
        "        'noise_std': 0.6,    # Bassa variabilità\n",
        "        'bias': -0.2         # Sottostimano leggermente\n",
        "    }\n",
        "}\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Spike/Deload Probabilities (Experience-Aware)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "cfg.spike_deload_probs = {\n",
        "    'Beginner': {\n",
        "        'deload': 0.03,      # 3% (non sanno quando scaricare)\n",
        "        'spike': 0.20        # 20% (non sanno autoregolarsi)\n",
        "    },\n",
        "    'Intermediate': {\n",
        "        'deload': 0.05,      # 5% (baseline)\n",
        "        'spike': 0.12        # 12% (buona gestione)\n",
        "    },\n",
        "    'Advanced': {\n",
        "        'deload': 0.08,      # 8% (programmano deload)\n",
        "        'spike': 0.06        # 6% (controllo ottimale)\n",
        "    }\n",
        "}\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Training History Duration (Experience-Aware)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "cfg.duration_days_by_level = {\n",
        "    'Beginner': (90, 270),       # 3-9 mesi\n",
        "    'Intermediate': (240, 600),  # 8-20 mesi\n",
        "    'Advanced': (540, 1080)      # 18-36 mesi\n",
        "}\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Consistency Target Ranges\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "cfg.consistency_targets = {\n",
        "    'Beginner': (0.65, 0.80),\n",
        "    'Intermediate': (0.75, 0.90),\n",
        "    'Advanced': (0.85, 0.95)\n",
        "}\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Progressive Overload Rates\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "cfg.overload_base_rate = {\n",
        "    'Beginner': 0.0500,    # +5% rate (newbie gains)\n",
        "    'Intermediate': 0.0050,\n",
        "    'Advanced': 0.0040\n",
        "}\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# User Profiles (Independent da experience)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "USER_PROFILES = {\n",
        "    'balanced': {\n",
        "        'desc': \"Equilibrato volume/intensità\",\n",
        "        'sets_mult': 1.0,\n",
        "        'rpe_mult': 1.0,\n",
        "        'skip_mult': 1.0,\n",
        "        'discipline_base': 0.75\n",
        "    },\n",
        "    'high_volume': {\n",
        "        'desc': \"Alto volume, intensità moderata\",\n",
        "        'sets_mult': 1.35,\n",
        "        'rpe_mult': 0.80,\n",
        "        'skip_mult': 1.15,\n",
        "        'discipline_base': 0.65\n",
        "    },\n",
        "    'high_intensity': {\n",
        "        'desc': \"Basso volume, alta intensità\",\n",
        "        'sets_mult': 0.75,\n",
        "        'rpe_mult': 1.25,\n",
        "        'skip_mult': 0.85,\n",
        "        'discipline_base': 0.80\n",
        "    },\n",
        "    'inconsistent': {\n",
        "        'desc': \"Irregolare, skippa spesso\",\n",
        "        'sets_mult': 1.0,\n",
        "        'rpe_mult': 0.95,\n",
        "        'skip_mult': 2.5,\n",
        "        'discipline_base': 0.45\n",
        "    }\n",
        "}\n",
        "\n",
        "cfg.user_profiles = USER_PROFILES\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CONFIGURATION v2.0 LOADED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"[OK] Users: {cfg.n_users}\")\n",
        "print(f\"[OK] Seed: {cfg.seed}\")\n",
        "print(f\"[OK] Output dir: {cfg.out_dir}\")\n",
        "print(\"\\n[OK] Experience-Aware Parameters:\")\n",
        "print(f\"  - Skip rates: {cfg.skip_p0_by_level}\")\n",
        "print(f\"  - RPE calibration: {len(cfg.rpe_params_by_level)} levels\")\n",
        "print(f\"  - Spike probs: {cfg.spike_deload_probs}\")\n",
        "print(f\"  - Duration ranges: {cfg.duration_days_by_level}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create RNG\n",
        "rng = np.random.default_rng(cfg.seed)\n",
        "\n",
        "# Create output directory\n",
        "OUTDIR = Path(cfg.out_dir)\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\n[OK] RNG initialized with seed {cfg.seed}\")\n",
        "print(f\"[OK] Output directory created: {OUTDIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohVOjSCCyVsx",
        "outputId": "e777c31f-ca1a-4191-d203-5a71e9f3bc53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CONFIGURATION v2.0 LOADED\n",
            "================================================================================\n",
            "[OK] Users: 1500\n",
            "[OK] Seed: 99999\n",
            "[OK] Output dir: data/synth_set_level_v2\n",
            "\n",
            "[OK] Experience-Aware Parameters:\n",
            "  - Skip rates: {'Beginner': 0.13, 'Intermediate': 0.075, 'Advanced': 0.05}\n",
            "  - RPE calibration: 3 levels\n",
            "  - Spike probs: {'Beginner': {'deload': 0.03, 'spike': 0.2}, 'Intermediate': {'deload': 0.05, 'spike': 0.12}, 'Advanced': {'deload': 0.08, 'spike': 0.06}}\n",
            "  - Duration ranges: {'Beginner': (90, 270), 'Intermediate': (240, 600), 'Advanced': (540, 1080)}\n",
            "================================================================================\n",
            "\n",
            "[OK] RNG initialized with seed 99999\n",
            "[OK] Output directory created: data/synth_set_level_v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 4 - Utility Functions**"
      ],
      "metadata": {
        "id": "QgeDQZ3ayN_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# UTILITY FUNCTIONS\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def sigmoid(z: float) -> float:\n",
        "    \"\"\"Sigmoid function for probability.\"\"\"\n",
        "    return 1.0 / (1.0 + math.exp(-z))\n",
        "\n",
        "def logit(p: float) -> float:\n",
        "    \"\"\"Inverse sigmoid: logit(p) = ln(p/(1-p))\"\"\"\n",
        "    return math.log(p / (1 - p))\n",
        "\n",
        "def q_load(x: float, step: float) -> float:\n",
        "    \"\"\"Quantize load to nearest step (e.g., 0.25 kg).\"\"\"\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return np.nan\n",
        "    return float(np.round(x / step) * step)\n",
        "\n",
        "def q_rpe(x: float, step: float) -> float:\n",
        "    \"\"\"Quantize RPE to nearest step (e.g., 0.5) and clip [1, 10].\"\"\"\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return np.nan\n",
        "    x = float(np.clip(x, 1.0, 10.0))\n",
        "    return float(np.round(x / step) * step)\n",
        "\n",
        "def clamp_int(x, lo, hi):\n",
        "    \"\"\"Clamp and cast to int.\"\"\"\n",
        "    return int(np.clip(int(round(x)), lo, hi))\n",
        "\n",
        "def sample_split(rng):\n",
        "    \"\"\"Sample split type: PPL (70%) or FullBody (30%).\"\"\"\n",
        "    return str(rng.choice(['PPL', 'FullBody'], p=[0.7, 0.3]))\n",
        "\n",
        "def exp_weights(L: int, tau: float) -> np.ndarray:\n",
        "    \"\"\"Exponential weights for Banister model.\"\"\"\n",
        "    idx = np.arange(L, dtype=float)\n",
        "    return np.exp(-idx / float(tau))\n",
        "\n",
        "print(\"[OK] Utility functions loaded (8 functions)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IINbb3-_yk8H",
        "outputId": "34afeebf-7851-40a6-a7b3-4a7a68934920"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Utility functions loaded (8 functions)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 5 - Exercise Catalog**"
      ],
      "metadata": {
        "id": "Hsvviv1tyOBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# EXERCISE CATALOG (Unchanged from v1.0)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "EXERCISES = [\n",
        "    # Push exercises\n",
        "    {\"exercise_id\": 1, \"exercise_name\": \"Bench Press\", \"target_muscle_group\": \"Chest\", \"split_cat\": \"Push\"},\n",
        "    {\"exercise_id\": 2, \"exercise_name\": \"Incline Dumbbell Press\", \"target_muscle_group\": \"Chest\", \"split_cat\": \"Push\"},\n",
        "    {\"exercise_id\": 3, \"exercise_name\": \"Overhead Press\", \"target_muscle_group\": \"Shoulders\", \"split_cat\": \"Push\"},\n",
        "    {\"exercise_id\": 4, \"exercise_name\": \"Lateral Raises\", \"target_muscle_group\": \"Shoulders\", \"split_cat\": \"Push\"},\n",
        "    {\"exercise_id\": 5, \"exercise_name\": \"Tricep Dips\", \"target_muscle_group\": \"Triceps\", \"split_cat\": \"Push\"},\n",
        "    {\"exercise_id\": 6, \"exercise_name\": \"Tricep Pushdowns\", \"target_muscle_group\": \"Triceps\", \"split_cat\": \"Push\"},\n",
        "\n",
        "    # Pull exercises\n",
        "    {\"exercise_id\": 7, \"exercise_name\": \"Pull-ups\", \"target_muscle_group\": \"Back\", \"split_cat\": \"Pull\"},\n",
        "    {\"exercise_id\": 8, \"exercise_name\": \"Bent-Over Rows\", \"target_muscle_group\": \"Back\", \"split_cat\": \"Pull\"},\n",
        "    {\"exercise_id\": 9, \"exercise_name\": \"Lat Pulldowns\", \"target_muscle_group\": \"Back\", \"split_cat\": \"Pull\"},\n",
        "    {\"exercise_id\": 10, \"exercise_name\": \"Face Pulls\", \"target_muscle_group\": \"Back\", \"split_cat\": \"Pull\"},\n",
        "    {\"exercise_id\": 11, \"exercise_name\": \"Barbell Curls\", \"target_muscle_group\": \"Biceps\", \"split_cat\": \"Pull\"},\n",
        "    {\"exercise_id\": 12, \"exercise_name\": \"Hammer Curls\", \"target_muscle_group\": \"Biceps\", \"split_cat\": \"Pull\"},\n",
        "\n",
        "    # Legs exercises\n",
        "    {\"exercise_id\": 13, \"exercise_name\": \"Squats\", \"target_muscle_group\": \"Quads\", \"split_cat\": \"Legs\"},\n",
        "    {\"exercise_id\": 14, \"exercise_name\": \"Leg Press\", \"target_muscle_group\": \"Quads\", \"split_cat\": \"Legs\"},\n",
        "    {\"exercise_id\": 15, \"exercise_name\": \"Lunges\", \"target_muscle_group\": \"Quads\", \"split_cat\": \"Legs\"},\n",
        "    {\"exercise_id\": 16, \"exercise_name\": \"Romanian Deadlifts\", \"target_muscle_group\": \"Hamstrings\", \"split_cat\": \"Legs\"},\n",
        "    {\"exercise_id\": 17, \"exercise_name\": \"Leg Curls\", \"target_muscle_group\": \"Hamstrings\", \"split_cat\": \"Legs\"},\n",
        "    {\"exercise_id\": 18, \"exercise_name\": \"Calf Raises\", \"target_muscle_group\": \"Calves\", \"split_cat\": \"Legs\"},\n",
        "\n",
        "    # FullBody compound\n",
        "    {\"exercise_id\": 19, \"exercise_name\": \"Deadlifts\", \"target_muscle_group\": \"Back\", \"split_cat\": \"FullBody\"},\n",
        "    {\"exercise_id\": 20, \"exercise_name\": \"Power Cleans\", \"target_muscle_group\": \"Full\", \"split_cat\": \"FullBody\"},\n",
        "]\n",
        "\n",
        "df_exercises = pd.DataFrame(EXERCISES)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXERCISE CATALOG LOADED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"[OK] Total exercises: {len(df_exercises)}\")\n",
        "print(f\"[OK] Split categories: {df_exercises['split_cat'].unique().tolist()}\")\n",
        "print(f\"[OK] Muscle groups: {df_exercises['target_muscle_group'].nunique()}\")\n",
        "print(\"\\nPreview:\")\n",
        "print(df_exercises.head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra54mMgcy_jB",
        "outputId": "364e884d-743d-45ee-88cb-031636cfe29e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "EXERCISE CATALOG LOADED\n",
            "================================================================================\n",
            "[OK] Total exercises: 20\n",
            "[OK] Split categories: ['Push', 'Pull', 'Legs', 'FullBody']\n",
            "[OK] Muscle groups: 9\n",
            "\n",
            "Preview:\n",
            " exercise_id          exercise_name target_muscle_group split_cat\n",
            "           1            Bench Press               Chest      Push\n",
            "           2 Incline Dumbbell Press               Chest      Push\n",
            "           3         Overhead Press           Shoulders      Push\n",
            "           4         Lateral Raises           Shoulders      Push\n",
            "           5            Tricep Dips             Triceps      Push\n",
            "           6       Tricep Pushdowns             Triceps      Push\n",
            "           7               Pull-ups                Back      Pull\n",
            "           8         Bent-Over Rows                Back      Pull\n",
            "           9          Lat Pulldowns                Back      Pull\n",
            "          10             Face Pulls                Back      Pull\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 6 - Prescribe Exercise**"
      ],
      "metadata": {
        "id": "jSIb8fG2yODw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# PRESCRIBE EXERCISE FUNCTION (Unchanged from v1.0)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def prescribe_exercise(ex_row: dict, experience_label: str, rng) -> tuple:\n",
        "    \"\"\"\n",
        "    Prescrizioni set/reps/rest/RIR per esercizio basate su experience.\n",
        "\n",
        "    Returns:\n",
        "        (sets_planned, reps_min, reps_max, rest_sec, rir_target)\n",
        "    \"\"\"\n",
        "\n",
        "    # Sets planned based on experience\n",
        "    sets_by_level = {\n",
        "        'Beginner': (2, 4),\n",
        "        'Intermediate': (3, 5),\n",
        "        'Advanced': (4, 6)\n",
        "    }\n",
        "    sets_min, sets_max = sets_by_level.get(experience_label, (3, 5))\n",
        "    sets_planned = int(rng.integers(sets_min, sets_max + 1))\n",
        "\n",
        "    # Reps range\n",
        "    reps_min = 6\n",
        "    reps_max = 12\n",
        "\n",
        "    # Rest time (seconds)\n",
        "    rest_sec = int(rng.choice([90, 120, 150, 180]))\n",
        "\n",
        "    # RIR target\n",
        "    rir_by_level = {\n",
        "        'Beginner': (2, 4),\n",
        "        'Intermediate': (1, 3),\n",
        "        'Advanced': (0, 2)\n",
        "    }\n",
        "    rir_min, rir_max = rir_by_level.get(experience_label, (1, 3))\n",
        "    rir_target = int(rng.integers(rir_min, rir_max + 1))\n",
        "\n",
        "    return sets_planned, reps_min, reps_max, rest_sec, rir_target\n",
        "\n",
        "print(\"[OK] prescribe_exercise() function loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzmRjJz3zPci",
        "outputId": "12e8dca5-7a2d-425a-df6a-c54ca14a9dfb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] prescribe_exercise() function loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 7 - Intensity from Reps/RIR**"
      ],
      "metadata": {
        "id": "tHx7pMV0yOF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# INTENSITY FROM REPS/RIR (Unchanged from v1.0)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def intensity_from_reps_rir(reps_target: int, rir: int, rng) -> float:\n",
        "    \"\"\"\n",
        "    Stima intensità relativa (% 1RM) da reps target e RIR.\n",
        "\n",
        "    Formula empirica: intensity = 1.0 - 0.025 * (reps + rir - 1)\n",
        "    Noise: ±3% per variabilità intra-individuale\n",
        "    \"\"\"\n",
        "\n",
        "    # Base formula (Epley-based approximation)\n",
        "    inten = 1.0 - 0.025 * (reps_target + rir - 1)\n",
        "\n",
        "    # Add noise\n",
        "    inten = float(rng.normal(inten, 0.03))\n",
        "\n",
        "    # Clamp [0.5, 1.0]\n",
        "    inten = float(np.clip(inten, 0.5, 1.0))\n",
        "\n",
        "    return inten\n",
        "\n",
        "# Test\n",
        "test_intensity = intensity_from_reps_rir(8, 2, rng)\n",
        "print(f\"[OK] intensity_from_reps_rir() function loaded\")\n",
        "print(f\"  Test: 8 reps @ 2 RIR → {test_intensity:.2%} 1RM\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWO9b7pCzVAP",
        "outputId": "87d9f94a-974f-46b2-f3c5-a1bf24cd1247"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] intensity_from_reps_rir() function loaded\n",
            "  Test: 8 reps @ 2 RIR → 79.95% 1RM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 8 - User Generation v2.0 (Duration Stratificata)**"
      ],
      "metadata": {
        "id": "1Ss6PFyQyOIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# USER GENERATION v2.0 (Experience-Aware Duration)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"USER GENERATION v2.0 (Experience-Aware Training History)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Experience distribution (realistico: ~35% Beginner, ~52% Intermediate, ~13% Advanced)\n",
        "experience_probs = [0.36, 0.52, 0.12]\n",
        "experience_labels = rng.choice(\n",
        "    ['Beginner', 'Intermediate', 'Advanced'],\n",
        "    size=cfg.n_users,\n",
        "    p=experience_probs\n",
        ")\n",
        "\n",
        "print(f\"\\n[OK] Experience distribution (n={cfg.n_users}):\")\n",
        "exp_counts = pd.Series(experience_labels).value_counts().sort_index()\n",
        "for label, count in exp_counts.items():\n",
        "    pct = count / cfg.n_users * 100\n",
        "    print(f\"  {label:12s}: {count:4d} ({pct:5.1f}%)\")\n",
        "\n",
        "# Weekly frequency\n",
        "weekly_freqs = rng.normal(cfg.weekly_freq_mu, cfg.weekly_freq_sd, size=cfg.n_users)\n",
        "weekly_freqs = np.clip(weekly_freqs, cfg.weekly_freq_min, cfg.weekly_freq_max)\n",
        "\n",
        "# User profiles (uniform distribution)\n",
        "profile_names = list(cfg.user_profiles.keys())\n",
        "profiles = rng.choice(profile_names, size=cfg.n_users)\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# DURATION STRATIFICATA PER EXPERIENCE (v2.0)\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "def generate_user_duration(experience_label: str, rng) -> int:\n",
        "    \"\"\"\n",
        "    Genera durata finestra temporale training basata su esperienza.\n",
        "\n",
        "    Rationale (Rønnestad & Mujika 2014):\n",
        "    - Beginner: storia breve (3-9 mesi)\n",
        "    - Intermediate: storia media (8-20 mesi)\n",
        "    - Advanced: storia lunga (18-36 mesi)\n",
        "    \"\"\"\n",
        "    min_days, max_days = cfg.duration_days_by_level[experience_label]\n",
        "    duration = int(rng.integers(min_days, max_days + 1))\n",
        "    return duration\n",
        "\n",
        "# Generate start/end dates per user\n",
        "user_rows = []\n",
        "\n",
        "for i in range(cfg.n_users):\n",
        "    uid = i + 1\n",
        "    exp_label = experience_labels[i]\n",
        "\n",
        "    # v2.0: Duration experience-aware\n",
        "    duration_days = generate_user_duration(exp_label, rng)\n",
        "\n",
        "    # Start date: today - duration\n",
        "    start_date = cfg.today - timedelta(days=duration_days)\n",
        "    end_date = cfg.today\n",
        "\n",
        "    # Weekly frequency\n",
        "    wf = float(weekly_freqs[i])\n",
        "    wf_declared = clamp_int(wf, cfg.weekly_freq_min, cfg.weekly_freq_max)\n",
        "\n",
        "    # Profile\n",
        "    profile = profiles[i]\n",
        "    profile_dict = cfg.user_profiles[profile]\n",
        "\n",
        "    # User traits (latenti)\n",
        "    exp_latent = float(rng.uniform(0.0, 1.0))\n",
        "    alpha_adapt = float(rng.uniform(0.03, 0.12))\n",
        "    k_detraining = float(rng.uniform(0.005, 0.02))\n",
        "    obs_noise = float(rng.uniform(0.5, 1.5))\n",
        "    resilience = float(rng.uniform(0.5, 1.2))\n",
        "    fatigue_sens = float(rng.uniform(0.6, 1.4))\n",
        "    rpe_report_bias = float(rng.uniform(-0.3, 0.3))\n",
        "\n",
        "    # Discipline e motivation\n",
        "    discipline_base = profile_dict['discipline_base']\n",
        "    discipline = float(np.clip(rng.normal(discipline_base, 0.15), 0.1, 1.0))\n",
        "    motivation = float(np.clip(rng.normal(0.7, 0.15), 0.3, 1.0))\n",
        "\n",
        "    user_rows.append({\n",
        "        'user_id': uid,\n",
        "        'start_date': start_date.isoformat(),\n",
        "        'end_date': end_date.isoformat(),\n",
        "        'duration_days': duration_days,\n",
        "        'weekly_freq_declared': wf_declared,\n",
        "        'split_type': sample_split(rng),\n",
        "        'profile': profile,\n",
        "        'experience_label': exp_label,\n",
        "        'experience_latent': exp_latent,\n",
        "        'alpha_adapt': alpha_adapt,\n",
        "        'k_detraining': k_detraining,\n",
        "        'obs_noise': obs_noise,\n",
        "        'resilience': resilience,\n",
        "        'fatigue_sens': fatigue_sens,\n",
        "        'rpe_report_bias': rpe_report_bias,\n",
        "        'discipline': discipline,\n",
        "        'motivation': motivation\n",
        "    })\n",
        "\n",
        "df_users = pd.DataFrame(user_rows)\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"USER DATAFRAME CREATED\")\n",
        "print(\"-\"*80)\n",
        "print(f\"[OK] Shape: {df_users.shape}\")\n",
        "print(f\"[OK] Columns: {df_users.columns.tolist()}\")\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# DURATION STATISTICS PER EXPERIENCE (v2.0 Validation)\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"DURATION STATISTICS per EXPERIENCE LEVEL\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for exp_label in ['Beginner', 'Intermediate', 'Advanced']:\n",
        "    subset = df_users[df_users['experience_label'] == exp_label]['duration_days']\n",
        "    mean_days = subset.mean()\n",
        "    mean_months = mean_days / 30.0\n",
        "    min_days = subset.min()\n",
        "    max_days = subset.max()\n",
        "\n",
        "    print(f\"{exp_label:12s}: {mean_days:6.0f} days ({mean_months:5.1f} months) | Range: [{min_days}-{max_days}]\")\n",
        "\n",
        "# Total sets estimation\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ESTIMATED TOTAL SETS per EXPERIENCE LEVEL\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for exp_label in ['Beginner', 'Intermediate', 'Advanced']:\n",
        "    subset = df_users[df_users['experience_label'] == exp_label]\n",
        "    avg_duration = subset['duration_days'].mean()\n",
        "    avg_weekly_freq = subset['weekly_freq_declared'].mean()\n",
        "\n",
        "    # Estimate: weeks * freq * sets_per_session (assume ~15 sets/session)\n",
        "    weeks = avg_duration / 7.0\n",
        "    estimated_total_sets = weeks * avg_weekly_freq * 15\n",
        "\n",
        "    print(f\"{exp_label:12s}: ~{estimated_total_sets:6.0f} total sets (avg)\")\n",
        "\n",
        "print(\"\\n[OK] User generation v2.0 complete\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCpUuDUlzbT4",
        "outputId": "9549dce8-4e42-4e6a-8f0b-14ceee01bd19"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "USER GENERATION v2.0 (Experience-Aware Training History)\n",
            "================================================================================\n",
            "\n",
            "[OK] Experience distribution (n=1500):\n",
            "  Advanced    :  189 ( 12.6%)\n",
            "  Beginner    :  542 ( 36.1%)\n",
            "  Intermediate:  769 ( 51.3%)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "USER DATAFRAME CREATED\n",
            "--------------------------------------------------------------------------------\n",
            "[OK] Shape: (1500, 17)\n",
            "[OK] Columns: ['user_id', 'start_date', 'end_date', 'duration_days', 'weekly_freq_declared', 'split_type', 'profile', 'experience_label', 'experience_latent', 'alpha_adapt', 'k_detraining', 'obs_noise', 'resilience', 'fatigue_sens', 'rpe_report_bias', 'discipline', 'motivation']\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DURATION STATISTICS per EXPERIENCE LEVEL\n",
            "--------------------------------------------------------------------------------\n",
            "Beginner    :    182 days (  6.1 months) | Range: [90-270]\n",
            "Intermediate:    423 days ( 14.1 months) | Range: [240-600]\n",
            "Advanced    :    796 days ( 26.5 months) | Range: [545-1078]\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ESTIMATED TOTAL SETS per EXPERIENCE LEVEL\n",
            "--------------------------------------------------------------------------------\n",
            "Beginner    : ~  1365 total sets (avg)\n",
            "Intermediate: ~  3128 total sets (avg)\n",
            "Advanced    : ~  6035 total sets (avg)\n",
            "\n",
            "[OK] User generation v2.0 complete\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 9 - ACWR Calculator v2.0 (Spike Experience-Aware)**"
      ],
      "metadata": {
        "id": "raXCa9WayOKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# ACWR CALCULATOR v2.0 (Experience-Aware Spike/Deload)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def calculate_weekly_acwr_multiplier(cfg, week_impulses: list, rng, experience_label: str):\n",
        "    \"\"\"\n",
        "    Calcola ACWR multiplier con probabilità spike/deload modulata\n",
        "    da livello esperienza (auto-regolazione).\n",
        "\n",
        "    Args:\n",
        "        cfg: Configuration object\n",
        "        week_impulses: Lista impulsi settimanali storici\n",
        "        rng: Random generator\n",
        "        experience_label: 'Beginner' | 'Intermediate' | 'Advanced'\n",
        "\n",
        "    Returns:\n",
        "        (multiplier, week_type, acwr_value)\n",
        "    \"\"\"\n",
        "\n",
        "    if not cfg.spike_enabled:\n",
        "        return 1.05, 'normal', 1.05\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # Chronic load (media ultime 4 settimane)\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    if len(week_impulses) >= 4:\n",
        "        chronic_load = float(np.mean(week_impulses[-4:]))\n",
        "    elif len(week_impulses) > 0:\n",
        "        chronic_load = float(np.mean(week_impulses))\n",
        "    else:\n",
        "        chronic_load = cfg.overload_I0\n",
        "\n",
        "    if chronic_load < 100.0:\n",
        "        chronic_load = cfg.overload_I0\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # PROBABILITÀ SPIKE/DELOAD EXPERIENCE-AWARE (v2.0)\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    probs = cfg.spike_deload_probs[experience_label]\n",
        "\n",
        "    # Decisione tipo settimana\n",
        "    rand = float(rng.random())\n",
        "\n",
        "    if rand < probs['deload']:\n",
        "        # DELOAD\n",
        "        acwr = float(rng.uniform(cfg.deload_acwr_min, cfg.deload_acwr_max))\n",
        "        week_type = 'deload'\n",
        "\n",
        "    elif rand < probs['deload'] + probs['spike']:\n",
        "        # SPIKE\n",
        "        acwr = float(rng.uniform(cfg.spike_acwr_min, cfg.spike_acwr_max))\n",
        "        week_type = 'spike'\n",
        "\n",
        "    else:\n",
        "        # NORMAL\n",
        "        acwr = float(rng.normal(cfg.normal_acwr_mean, cfg.normal_acwr_std))\n",
        "        acwr = float(np.clip(acwr, 0.90, 1.25))\n",
        "        week_type = 'normal'\n",
        "\n",
        "    multiplier = acwr\n",
        "\n",
        "    return multiplier, week_type, acwr\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ACWR CALCULATOR v2.0 LOADED\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n[OK] Experience-Aware Spike/Deload Probabilities:\")\n",
        "for level, probs in cfg.spike_deload_probs.items():\n",
        "    spike_pct = probs['spike'] * 100\n",
        "    deload_pct = probs['deload'] * 100\n",
        "    normal_pct = (1 - probs['spike'] - probs['deload']) * 100\n",
        "    print(f\"  {level:12s}: Spike {spike_pct:4.0f}% | Deload {deload_pct:4.0f}% | Normal {normal_pct:4.0f}%\")\n",
        "\n",
        "# Test function\n",
        "test_multiplier, test_type, test_acwr = calculate_weekly_acwr_multiplier(\n",
        "    cfg, [1800, 1900, 2000, 2100], rng, 'Beginner'\n",
        ")\n",
        "print(f\"\\n[OK] Test (Beginner): week_type={test_type}, ACWR={test_acwr:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi5eEK-Yz1G2",
        "outputId": "41a58a2f-ce82-492b-ac60-3b8206daf4a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ACWR CALCULATOR v2.0 LOADED\n",
            "================================================================================\n",
            "\n",
            "[OK] Experience-Aware Spike/Deload Probabilities:\n",
            "  Beginner    : Spike   20% | Deload    3% | Normal   77%\n",
            "  Intermediate: Spike   12% | Deload    5% | Normal   83%\n",
            "  Advanced    : Spike    6% | Deload    8% | Normal   86%\n",
            "\n",
            "[OK] Test (Beginner): week_type=spike, ACWR=1.790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 10 - Schedule Sessions**"
      ],
      "metadata": {
        "id": "J91deyCayOM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SCHEDULE SESSIONS FUNCTION (Unchanged from v1.0)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def schedule_sessions_for_user(start_u: date, end_u: date, weekly_freq: int, rng):\n",
        "    \"\"\"\n",
        "    Genera date sessioni per utente con jitter giornaliero.\n",
        "\n",
        "    Args:\n",
        "        start_u: Data inizio carriera utente\n",
        "        end_u: Data fine carriera utente\n",
        "        weekly_freq: Frequenza settimanale target\n",
        "        rng: Random generator\n",
        "\n",
        "    Returns:\n",
        "        Lista date sessioni (sorted, unique)\n",
        "    \"\"\"\n",
        "\n",
        "    # Giorni target della settimana (base pattern)\n",
        "    base_days = sorted(rng.choice(np.arange(7), size=weekly_freq, replace=False).tolist())\n",
        "\n",
        "    dates = []\n",
        "    d0 = start_u\n",
        "    n_days = (end_u - start_u).days + 1\n",
        "\n",
        "    for i in range(n_days):\n",
        "        day = d0 + timedelta(days=i)\n",
        "\n",
        "        if day.weekday() in base_days:\n",
        "            # Jitter: -1, 0, +1 giorno\n",
        "            jitter = int(rng.choice([-1, 0, 1], p=cfg.weekday_jitter_probs))\n",
        "            day2 = day + timedelta(days=jitter)\n",
        "\n",
        "            # Keep in range\n",
        "            if start_u <= day2 <= end_u:\n",
        "                dates.append(day2)\n",
        "\n",
        "    # Remove duplicates and sort\n",
        "    dates = sorted(list(set(dates)))\n",
        "\n",
        "    return dates\n",
        "\n",
        "# Test function\n",
        "test_dates = schedule_sessions_for_user(\n",
        "    date(2025, 1, 1),\n",
        "    date(2025, 1, 31),\n",
        "    3,\n",
        "    rng\n",
        ")\n",
        "print(\"=\"*80)\n",
        "print(\"SCHEDULE SESSIONS FUNCTION LOADED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"[OK] Test (Jan 2025, 3x/week): {len(test_dates)} sessions scheduled\")\n",
        "print(f\"  First 5 dates: {test_dates[:5]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmVWWDKkz9zq",
        "outputId": "a23e48a6-9af9-4750-dcf7-16f9ceae16a8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SCHEDULE SESSIONS FUNCTION LOADED\n",
            "================================================================================\n",
            "[OK] Test (Jan 2025, 3x/week): 13 sessions scheduled\n",
            "  First 5 dates: [datetime.date(2025, 1, 1), datetime.date(2025, 1, 4), datetime.date(2025, 1, 7), datetime.date(2025, 1, 8), datetime.date(2025, 1, 11)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 11 - Simulate User v2.0 (CORE LOGIC - Tutte le modifiche integrate)**"
      ],
      "metadata": {
        "id": "KxRXQ7rZyOPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# SIMULATE USER v2.0 (Experience-Aware Skip + RPE)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "def simulate_user(cfg, user_row: dict, df_ex: pd.DataFrame, caps_u: dict, templates_u: dict, rng):\n",
        "    \"\"\"\n",
        "    Simula allenamento per singolo utente con progressive overload dinamico.\n",
        "\n",
        "    v2.0 Changes:\n",
        "    - Skip model experience-aware\n",
        "    - RPE calibration experience-aware\n",
        "    - Spike frequency experience-aware\n",
        "\n",
        "    Returns:\n",
        "        (workouts_rows, plan_rows, sets_rows, impulse_rows, user_meta)\n",
        "    \"\"\"\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # USER PARAMETERS\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    uid = int(user_row['user_id'])\n",
        "    start_u = date.fromisoformat(user_row['start_date'])\n",
        "    end_u = date.fromisoformat(user_row['end_date'])\n",
        "    weekly_freq = int(user_row['weekly_freq_declared'])\n",
        "    experience_label = str(user_row['experience_label'])\n",
        "\n",
        "    # Latent traits\n",
        "    exp_lat = float(user_row['experience_latent'])\n",
        "    alpha = float(user_row['alpha_adapt'])\n",
        "    kd = float(user_row['k_detraining'])\n",
        "    obs_noise = float(user_row['obs_noise'])\n",
        "    resilience = float(user_row['resilience'])\n",
        "    fatigue_sens = float(user_row['fatigue_sens'])\n",
        "    rpe_bias = float(user_row['rpe_report_bias'])\n",
        "\n",
        "    # Profile & traits\n",
        "    profile_name = str(user_row.get('profile', 'balanced'))\n",
        "    profile = cfg.user_profiles.get(profile_name, cfg.user_profiles['balanced'])\n",
        "\n",
        "    user_discipline = float(user_row.get('discipline', 0.7))\n",
        "    user_motivation = float(user_row.get('motivation', 0.7))\n",
        "\n",
        "    # Banister params per utente\n",
        "    tau_F = float(max(7.0, rng.normal(cfg.tau_F_mean, cfg.tau_F_sd)))\n",
        "    tau_D = float(max(2.0, rng.normal(cfg.tau_D_mean, cfg.tau_D_sd)))\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # STATE INITIALIZATION\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    fitness = float(rng.normal(0.0, 1.0))\n",
        "    fatigue = float(max(0.0, rng.normal(0.5, 0.3)))\n",
        "    skill = float(np.clip(rng.normal(0.2 + 0.6 * exp_lat, 0.15), 0.0, 2.0))\n",
        "\n",
        "    injury_until = None\n",
        "\n",
        "    # Schedule candidato\n",
        "    session_dates = schedule_sessions_for_user(start_u, end_u, weekly_freq, rng)\n",
        "\n",
        "    workouts_rows = []\n",
        "    plan_rows = []\n",
        "    sets_rows = []\n",
        "    impulse_rows = []\n",
        "\n",
        "    wid = 1\n",
        "    set_id_counter = 1\n",
        "\n",
        "    # Rotazione tag per split\n",
        "    PPL_ROT = ['Push', 'Pull', 'Legs']\n",
        "    FB_ROT = ['FullBody']\n",
        "    tags = PPL_ROT if str(user_row['split_type']) == 'PPL' else FB_ROT\n",
        "    tag_i = int(rng.integers(0, len(tags)))\n",
        "\n",
        "    last_train_date = None\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # PROGRESSIVE OVERLOAD STATE\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    current_caps = {eid: float(val) for eid, val in caps_u.items()}\n",
        "\n",
        "    base_growth = cfg.overload_base_rate.get(experience_label, 0.001)\n",
        "    growth_rate = float(np.clip(rng.normal(base_growth, 0.0002), 1e-5, 0.005))\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # SPIKE ACWR STATE\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    weekly_impulses = []\n",
        "    current_week_impulse = 0.0\n",
        "    current_week_start = start_u\n",
        "    week_type_current = 'normal'\n",
        "    acwr_multiplier = 1.05\n",
        "    acwr_value = 1.05\n",
        "    target_weekly_impulse = cfg.overload_I0\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # MAIN LOOP: PROCESS SESSIONS\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    for idx, d in enumerate(session_dates):\n",
        "\n",
        "        # ════════════════════════════════════════════════════\n",
        "        # SPIKE ACWR: Calcola allinizio settimana\n",
        "        # ════════════════════════════════════════════════════\n",
        "\n",
        "        week_index = (d - start_u).days // 7\n",
        "        week_start_for_d = start_u + timedelta(days=week_index * 7)\n",
        "\n",
        "        if week_start_for_d != current_week_start:\n",
        "            # Chiudi settimana precedente\n",
        "            if current_week_impulse > 0:\n",
        "                weekly_impulses.append(current_week_impulse)\n",
        "\n",
        "            # Calcola ACWR nuova settimana (v2.0: experience-aware)\n",
        "            acwr_multiplier, week_type_current, acwr_value = calculate_weekly_acwr_multiplier(\n",
        "                cfg, weekly_impulses, rng, experience_label\n",
        "            )\n",
        "\n",
        "            # Reset\n",
        "            current_week_impulse = 0.0\n",
        "            current_week_start = week_start_for_d\n",
        "\n",
        "            # Aggiorna target\n",
        "            if weekly_impulses:\n",
        "                chronic_avg = np.mean(weekly_impulses[-4:]) if len(weekly_impulses) >= 4 else np.mean(weekly_impulses)\n",
        "                target_weekly_impulse = chronic_avg * acwr_multiplier\n",
        "            else:\n",
        "                target_weekly_impulse = cfg.overload_I0 * acwr_multiplier\n",
        "\n",
        "        # ════════════════════════════════════════════════════\n",
        "        # DETRAINING (se gap > 1 giorno)\n",
        "        # ════════════════════════════════════════════════════\n",
        "\n",
        "        if last_train_date is not None:\n",
        "            gap = (d - last_train_date).days\n",
        "            if gap > 1:\n",
        "                fitness *= math.exp(-kd * gap)\n",
        "\n",
        "                # Detraining anche su capacità se gap molto lungo (>7 giorni)\n",
        "                if gap > 7:\n",
        "                    decay = math.exp(-kd * (gap - 7) * 0.3)\n",
        "                    for eid in current_caps:\n",
        "                        current_caps[eid] *= decay\n",
        "\n",
        "        # Decay fatica giornaliero\n",
        "        fatigue *= math.exp(-1.0 / 7.0)\n",
        "\n",
        "        # ════════════════════════════════════════════════════\n",
        "        # INJURY CHECK\n",
        "        # ════════════════════════════════════════════════════\n",
        "\n",
        "        in_injury = injury_until is not None and d <= injury_until\n",
        "\n",
        "        # ════════════════════════════════════════════════════\n",
        "        # SKIP MODEL v2.0 (EXPERIENCE-AWARE)\n",
        "        # ════════════════════════════════════════════════════\n",
        "\n",
        "        p0_base = float(cfg.skip_p0_by_level.get(experience_label, 0.10))\n",
        "\n",
        "        # Discipline MODULA invece di SOSTITUIRE (v2.0)\n",
        "        p0 = p0_base * (1.5 - 0.5 * user_discipline)  # Range: [1.0x, 1.5x] baseline\n",
        "        p0 = float(np.clip(p0, 0.01, 0.25))\n",
        "\n",
        "        bias = logit(p0)\n",
        "\n",
        "        # Fatigue sensitivity per livello (v2.0)\n",
        "        fatigue_weight = cfg.skip_fatigue_sensitivity[experience_label]\n",
        "\n",
        "        fat_term = float(np.log1p(max(0.0, float(fatigue))))\n",
        "        fat_term = min(fat_term, float(cfg.skip_fatigue_cap))\n",
        "\n",
        "        z = bias + fatigue_weight * fat_term + float(rng.normal(0.0, cfg.skip_noise_sd))\n",
        "        p_skip = sigmoid(z)\n",
        "\n",
        "        status = 'done'\n",
        "        if rng.random() < p_skip:\n",
        "            status = 'skipped'\n",
        "\n",
        "        # ════════════════════════════════════════════════════\n",
        "        # SESSION TAG\n",
        "        # ════════════════════════════════════════════════════\n",
        "\n",
        "        tag = tags[tag_i % len(tags)]\n",
        "        tag_i += 1\n",
        "\n",
        "        week_index_user = (d - start_u).days // 7 + 1\n",
        "\n",
        "        # ════════════════════════════════════════════════════\n",
        "        # WORKOUTS ROW\n",
        "        # ════════════════════════════════════════════════════\n",
        "\n",
        "        workouts_rows.append({\n",
        "            'user_id': uid,\n",
        "            'date': d.isoformat(),\n",
        "            'week_index_user': int(week_index_user),\n",
        "            'session_tag': tag,\n",
        "            'workout_status': status,\n",
        "            'z_skip': float(z),\n",
        "            'p_skip': float(p_skip),\n",
        "            'fatigue_term': float(fat_term),\n",
        "            'experience_label': experience_label,\n",
        "        })\n",
        "\n",
        "        if status == 'skipped':\n",
        "            impulse_rows.append({'user_id': uid, 'date': d.isoformat(), 'impulse': 0.0})\n",
        "            continue\n",
        "\n",
        "        # ════════════════════════════════════════════════════\n",
        "        # PLAN PER ESERCIZIO\n",
        "        # ════════════════════════════════════════════════════\n",
        "\n",
        "        ex_ids = templates_u.get(tag, [])\n",
        "\n",
        "        if len(ex_ids) == 0:\n",
        "            ex_ids = templates_u[list(templates_u.keys())[0]]\n",
        "\n",
        "        fatigue_session = float(fatigue)\n",
        "\n",
        "        day_impulse = 0.0\n",
        "        day_total_sets = 0\n",
        "\n",
        "        for ex_idx, ex_id in enumerate(ex_ids):\n",
        "            if day_total_sets > 100:  # Safety\n",
        "                break\n",
        "\n",
        "            ex_row = df_ex[df_ex['exercise_id'] == ex_id].iloc[0].to_dict()\n",
        "\n",
        "            sets_planned, reps_min, reps_max, rest_sec, rir_target = prescribe_exercise(\n",
        "                ex_row, experience_label, rng\n",
        "            )\n",
        "\n",
        "            # Volume reduction in injury\n",
        "            if in_injury:\n",
        "                sets_planned = max(1, int(round(sets_planned * 0.6)))\n",
        "\n",
        "            plan_rows.append({\n",
        "                'user_id': uid,\n",
        "                'date': d.isoformat(),\n",
        "                'session_tag': tag,\n",
        "                'exercise_id': int(ex_id),\n",
        "                'sets_planned': int(sets_planned),\n",
        "                'reps_min': int(reps_min),\n",
        "                'reps_max': int(reps_max),\n",
        "                'rest_planned_sec': int(rest_sec),\n",
        "                'rir_target': int(rir_target),\n",
        "            })\n",
        "\n",
        "            # ════════════════════════════════════════════════\n",
        "            # CAPACITY DINAMICA\n",
        "            # ════════════════════════════════════════════════\n",
        "\n",
        "            c_max = current_caps.get(int(ex_id), 50.0)\n",
        "\n",
        "            # Intended baseline load\n",
        "            reps_target_0 = int(rng.integers(reps_min, reps_max + 1))\n",
        "            inten_0 = intensity_from_reps_rir(reps_target_0, rir_target, rng)\n",
        "            intended_load = q_load(inten_0 * c_max, cfg.load_step)\n",
        "\n",
        "            # Sets done modulati da profilo + ACWR\n",
        "            sets_done_base = sets_planned * profile['sets_mult'] * acwr_multiplier\n",
        "            sets_done = int(np.clip(round(rng.normal(sets_done_base, 0.5)), 1, 10))\n",
        "\n",
        "            # ════════════════════════════════════════════════\n",
        "            # SET EXECUTION\n",
        "            # ════════════════════════════════════════════════\n",
        "\n",
        "            for s in range(1, sets_done + 1):\n",
        "                day_total_sets += 1\n",
        "\n",
        "                reps_target = int(rng.integers(reps_min, reps_max + 1))\n",
        "                inten = intensity_from_reps_rir(reps_target, rir_target, rng)\n",
        "\n",
        "                fatigue_factor = float(np.clip(0.03 * fatigue_session * fatigue_sens, 0.0, 0.20))\n",
        "\n",
        "                # Riduzione fatica per Beginner (newbie gains) - MANTIENI da v1.0\n",
        "                if experience_label == 'Beginner':\n",
        "                    fatigue_factor *= 0.2\n",
        "\n",
        "                # ════════════════════════════════════════════\n",
        "                # LOAD MODIFICATO v1.0 (MANTIENI)\n",
        "                # ════════════════════════════════════════════\n",
        "\n",
        "                load_from_capacity = inten * c_max * (1.0 - fatigue_factor)\n",
        "                daily_variation = rng.uniform(0.85, 1.15)\n",
        "                load_from_daily = c_max * 0.3 * daily_variation\n",
        "\n",
        "                load_done = load_from_capacity * 0.7 + load_from_daily * 0.3\n",
        "\n",
        "                # Rumore osservazionale\n",
        "                load_done = float(rng.normal(1.0, 0.04 * obs_noise * 0.10)) * load_done\n",
        "                load_done = q_load(float(np.clip(load_done, 2.5, c_max)), cfg.load_step)\n",
        "\n",
        "                # Reps calano se fatica sale\n",
        "                reps_done = int(np.clip(\n",
        "                    round(rng.normal(reps_target * (1.0 - 0.20 * fatigue_factor), 0.6 * obs_noise * 1.5)),\n",
        "                    1, 30\n",
        "                ))\n",
        "\n",
        "                # ════════════════════════════════════════════\n",
        "                # RPE MODIFICATO v2.0 (EXPERIENCE-AWARE)\n",
        "                # ════════════════════════════════════════════\n",
        "\n",
        "                rpe_params = cfg.rpe_params_by_level[experience_label]\n",
        "\n",
        "                # Componenti RPE\n",
        "                rpe_from_intensity = 5.0 + 4.0 * inten\n",
        "                rpe_from_motivation = user_motivation * 3.0\n",
        "                rpe_from_fatigue = fatigue_factor * 2.5\n",
        "\n",
        "                # Mix ponderato\n",
        "                rpe_true = (rpe_from_intensity * 0.5 +\n",
        "                            rpe_from_motivation * 0.3 +\n",
        "                            rpe_from_fatigue * 0.2)\n",
        "\n",
        "                # Observed RPE con NOISE e BIAS experience-aware (v2.0)\n",
        "                rpe_obs = rng.normal(\n",
        "                    rpe_true + rpe_bias + rpe_params['bias'],  # Bias livello\n",
        "                    rpe_params['noise_std'] * obs_noise * 1.2  # Noise livello\n",
        "                )\n",
        "\n",
        "                rpe_done = q_rpe(float(np.clip(rpe_obs, 1.0, 10.0)), cfg.rpe_step)\n",
        "\n",
        "                # Feedback testuale (opzionale, 5%)\n",
        "                feedback = None\n",
        "                if rng.random() < 0.05:\n",
        "                    feedback = str(rng.choice(['Recuperi corti', 'Sentito bene']))\n",
        "\n",
        "                # Missingness\n",
        "                if rng.random() < cfg.p_missing_load:\n",
        "                    load_done = np.nan\n",
        "                if rng.random() < cfg.p_missing_rpe:\n",
        "                    rpe_done = np.nan\n",
        "                if rng.random() < cfg.p_missing_feedback:\n",
        "                    feedback = None\n",
        "\n",
        "                # ════════════════════════════════════════════\n",
        "                # SETS ROW\n",
        "                # ════════════════════════════════════════════\n",
        "\n",
        "                sets_rows.append({\n",
        "                    'set_id': f\"{uid:04d}S{set_id_counter:07d}\",\n",
        "                    'user_id': uid,\n",
        "                    'date': d.isoformat(),\n",
        "                    'week_index_user': int(week_index_user),\n",
        "                    'week_type': week_type_current,\n",
        "                    'acwr': float(acwr_value),\n",
        "                    'session_tag': tag,\n",
        "                    'exercise_id': int(ex_id),\n",
        "                    'set_index': int(s),\n",
        "                    'reps_target': int(reps_target),\n",
        "                    'reps_done': int(reps_done),\n",
        "                    'load_intended_kg': float(intended_load),\n",
        "                    'load_done_kg': load_done,\n",
        "                    'rpe_done': rpe_done,\n",
        "                    'rest_planned_sec': int(rest_sec),\n",
        "                    'rir_target': int(rir_target),\n",
        "                    'feedback': feedback,\n",
        "                })\n",
        "\n",
        "                set_id_counter += 1\n",
        "\n",
        "                # Impulso giornaliero Banister\n",
        "                ld = 0.0 if isinstance(load_done, float) and np.isnan(load_done) else float(load_done)\n",
        "                rd = 0.0 if isinstance(rpe_done, float) and np.isnan(rpe_done) else float(rpe_done)\n",
        "                day_impulse += ld * float(reps_done) * rd / 10.0\n",
        "\n",
        "                # Accumula impulso settimanale\n",
        "                current_week_impulse += ld * float(reps_done) * rd / 10.0\n",
        "\n",
        "                # Aggiorna fatica intra-sessione\n",
        "                fatigue_session += 0.08 * inten + 0.02 * ld / max(20.0, c_max)\n",
        "\n",
        "        # ════════════════════════════════════════════════════\n",
        "        # PROGRESSIVE OVERLOAD POST-SESSIONE (MANTIENI v1.0)\n",
        "        # ════════════════════════════════════════════════════\n",
        "\n",
        "        stim = float(np.clip(day_impulse / cfg.overload_I0, 0.05, 2.0))\n",
        "        quality = max(0.2, 1.0 - cfg.overload_quality_fatigue * fatigue / 20.0)\n",
        "        gain_base = growth_rate * stim * quality\n",
        "\n",
        "        # Pre-calcola info esercizi\n",
        "        ex_info = {}\n",
        "        for eid in current_caps:\n",
        "            row = df_ex[df_ex['exercise_id'] == eid].iloc[0]\n",
        "            ex_info[eid] = {\n",
        "                'muscle': str(row['target_muscle_group']),\n",
        "                'split': str(row['split_cat']),\n",
        "            }\n",
        "\n",
        "        # Transfer per esercizi allenati\n",
        "        for ex_id_trained in ex_ids:\n",
        "            muscle_trained = ex_info[ex_id_trained]['muscle']\n",
        "            split_trained = ex_info[ex_id_trained]['split']\n",
        "\n",
        "            for eid_all in current_caps:\n",
        "                if eid_all == ex_id_trained:\n",
        "                    transfer_weight = 1.0\n",
        "                elif ex_info[eid_all]['muscle'] == muscle_trained:\n",
        "                    transfer_weight = cfg.transfer_same_muscle\n",
        "                elif ex_info[eid_all]['split'] == split_trained:\n",
        "                    transfer_weight = cfg.transfer_same_split\n",
        "                else:\n",
        "                    transfer_weight = 0.0\n",
        "\n",
        "                current_caps[eid_all] *= (1.0 + gain_base * transfer_weight)\n",
        "\n",
        "        # ════════════════════════════════════════════════════\n",
        "        # INJURY EVENT (MANTIENI v1.0)\n",
        "        # ════════════════════════════════════════════════════\n",
        "\n",
        "        p_injury = cfg.injury_lambda * (day_impulse / 1000.0) * (1.0 - resilience) * (1.0 + 0.5 * fatigue_sens)\n",
        "        if rng.random() < p_injury:\n",
        "            injury_days = int(rng.integers(cfg.injury_days_min, cfg.injury_days_max + 1))\n",
        "            injury_until = d + timedelta(days=injury_days)\n",
        "\n",
        "        # Aggiorna fatica globale\n",
        "        fatigue = float(np.clip(fatigue_session, 0.0, 20.0))\n",
        "\n",
        "        impulse_rows.append({'user_id': uid, 'date': d.isoformat(), 'impulse': float(day_impulse)})\n",
        "        last_train_date = d\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # USER METADATA (Banister params)\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    user_meta = {\n",
        "        'user_id': uid,\n",
        "        'tau_F': tau_F,\n",
        "        'tau_D': tau_D,\n",
        "        'beta_F': cfg.beta_F,\n",
        "        'beta_D': cfg.beta_D,\n",
        "    }\n",
        "\n",
        "    return workouts_rows, plan_rows, sets_rows, impulse_rows, user_meta\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SIMULATE USER v2.0 LOADED\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n[OK] Experience-Aware Components:\")\n",
        "print(\"  - Skip model: p0 stratificato per livello\")\n",
        "print(\"  - RPE calibration: noise + bias per livello\")\n",
        "print(\"  - Spike frequency: auto-regolazione esperti\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B9fTfvu0KEM",
        "outputId": "dbea61c2-6671-4c0f-dcc4-e25b29d0fb9c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SIMULATE USER v2.0 LOADED\n",
            "================================================================================\n",
            "\n",
            "[OK] Experience-Aware Components:\n",
            "  - Skip model: p0 stratificato per livello\n",
            "  - RPE calibration: noise + bias per livello\n",
            "  - Spike frequency: auto-regolazione esperti\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 12 - Exercise Templates & Capacities Setup**"
      ],
      "metadata": {
        "id": "bqDuG6bxyORc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# EXERCISE TEMPLATES & CAPACITIES SETUP (Unchanged from v1.0)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PREPARING EXERCISE TEMPLATES & CAPACITIES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Exercise Templates per Split\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "PUSH_EX = df_exercises[df_exercises['split_cat'] == 'Push']['exercise_id'].tolist()\n",
        "PULL_EX = df_exercises[df_exercises['split_cat'] == 'Pull']['exercise_id'].tolist()\n",
        "LEGS_EX = df_exercises[df_exercises['split_cat'] == 'Legs']['exercise_id'].tolist()\n",
        "FB_EX = df_exercises[df_exercises['split_cat'] == 'FullBody']['exercise_id'].tolist()\n",
        "\n",
        "# Fallback: se FullBody vuoto, usa compound\n",
        "if len(FB_EX) == 0:\n",
        "    FB_EX = [1, 7, 13]  # Bench, Pull-ups, Squats\n",
        "\n",
        "print(f\"[OK] Exercise pools:\")\n",
        "print(f\"  Push: {len(PUSH_EX)} exercises\")\n",
        "print(f\"  Pull: {len(PULL_EX)} exercises\")\n",
        "print(f\"  Legs: {len(LEGS_EX)} exercises\")\n",
        "print(f\"  FullBody: {len(FB_EX)} exercises\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Generate Templates & Capacities per User\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "def generate_templates_and_caps(user_row: dict, df_ex: pd.DataFrame, rng):\n",
        "    \"\"\"\n",
        "    Genera template esercizi e capacità iniziali per utente.\n",
        "    \"\"\"\n",
        "\n",
        "    split_type = user_row['split_type']\n",
        "    experience_label = user_row['experience_label']\n",
        "\n",
        "    # Sample esercizi per split\n",
        "    if split_type == 'PPL':\n",
        "        push_sample = list(rng.choice(PUSH_EX, size=min(4, len(PUSH_EX)), replace=False))\n",
        "        pull_sample = list(rng.choice(PULL_EX, size=min(4, len(PULL_EX)), replace=False))\n",
        "        legs_sample = list(rng.choice(LEGS_EX, size=min(4, len(LEGS_EX)), replace=False))\n",
        "\n",
        "        templates = {\n",
        "            'Push': push_sample,\n",
        "            'Pull': pull_sample,\n",
        "            'Legs': legs_sample\n",
        "        }\n",
        "    else:\n",
        "        fb_sample = list(rng.choice(FB_EX, size=min(5, len(FB_EX)), replace=False))\n",
        "        templates = {\n",
        "            'FullBody': fb_sample\n",
        "        }\n",
        "\n",
        "    # Capacità iniziali per TUTTI gli esercizi (per transfer)\n",
        "    caps = {}\n",
        "\n",
        "    # Capacity range per livello\n",
        "    cap_ranges = {\n",
        "        'Beginner': (25.0, 40.0),\n",
        "        'Intermediate': (40.0, 60.0),\n",
        "        'Advanced': (55.0, 80.0)\n",
        "    }\n",
        "\n",
        "    cap_min, cap_max = cap_ranges[experience_label]\n",
        "\n",
        "    for ex_id in df_ex['exercise_id']:\n",
        "        caps[int(ex_id)] = float(rng.uniform(cap_min, cap_max))\n",
        "\n",
        "    return templates, caps\n",
        "\n",
        "print(\"\\n[OK] generate_templates_and_caps() function ready\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osi6_5Bz0h5G",
        "outputId": "6683bbc6-c0a2-44db-94ae-2009d2f2c48d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PREPARING EXERCISE TEMPLATES & CAPACITIES\n",
            "================================================================================\n",
            "[OK] Exercise pools:\n",
            "  Push: 6 exercises\n",
            "  Pull: 6 exercises\n",
            "  Legs: 6 exercises\n",
            "  FullBody: 2 exercises\n",
            "\n",
            "[OK] generate_templates_and_caps() function ready\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 13 - Run Generator (Main Loop)**"
      ],
      "metadata": {
        "id": "HyIrK4z9yOTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# RUN GENERATOR - ALL USERS\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RUNNING GENERATOR v2.0 - ALL USERS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Users to process: {len(df_users)}\")\n",
        "print(f\"Estimated time: ~3-5 minutes\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "all_workouts = []\n",
        "all_plan = []\n",
        "all_sets = []\n",
        "all_impulse = []\n",
        "all_banister_meta = []\n",
        "\n",
        "skipped_users = []\n",
        "\n",
        "for idx, user_row in df_users.iterrows():\n",
        "    uid = user_row['user_id']\n",
        "\n",
        "    # Progress tracking (ogni 50 users)\n",
        "    if uid % 250 == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"[{uid:4d}/{len(df_users)}] Elapsed: {elapsed:.1f}s\")\n",
        "\n",
        "    # Generate templates & capacities\n",
        "    templates_u, caps_u = generate_templates_and_caps(user_row, df_exercises, rng)\n",
        "\n",
        "    # Simulate user\n",
        "    try:\n",
        "        workouts_rows, plan_rows, sets_rows, impulse_rows, user_meta = simulate_user(\n",
        "            cfg, user_row, df_exercises, caps_u, templates_u, rng\n",
        "        )\n",
        "\n",
        "        # Filter users con troppo pochi set (< 20)\n",
        "        if len(sets_rows) < 20:\n",
        "            skipped_users.append(uid)\n",
        "            continue\n",
        "\n",
        "        all_workouts.extend(workouts_rows)\n",
        "        all_plan.extend(plan_rows)\n",
        "        all_sets.extend(sets_rows)\n",
        "        all_impulse.extend(impulse_rows)\n",
        "        all_banister_meta.append(user_meta)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[!] ERROR processing user {uid}: {e}\")\n",
        "        skipped_users.append(uid)\n",
        "        continue\n",
        "\n",
        "elapsed_total = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"[OK] Time elapsed: {elapsed_total:.1f}s ({elapsed_total/60:.1f} min)\")\n",
        "print(f\"[OK] Users processed: {len(df_users) - len(skipped_users)}/{len(df_users)}\")\n",
        "if skipped_users:\n",
        "    print(f\"[!] Skipped users (< 20 sets): {len(skipped_users)}\")\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# CREATE DATAFRAMES\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "df_workouts = pd.DataFrame(all_workouts)\n",
        "df_plan = pd.DataFrame(all_plan)\n",
        "df_sets = pd.DataFrame(all_sets)\n",
        "df_impulse = pd.DataFrame(all_impulse)\n",
        "df_banister_meta = pd.DataFrame(all_banister_meta)\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"DATAFRAMES CREATED\")\n",
        "print(\"-\"*80)\n",
        "print(f\"[OK] df_workouts: {len(df_workouts):,} rows\")\n",
        "print(f\"[OK] df_plan:     {len(df_plan):,} rows\")\n",
        "print(f\"[OK] df_sets:     {len(df_sets):,} rows (PRIMARY)\")\n",
        "print(f\"[OK] df_impulse:  {len(df_impulse):,} rows\")\n",
        "print(f\"[OK] df_banister_meta: {len(df_banister_meta):,} rows\")\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# ASSIGN GLOBAL WORKOUT IDs\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ASSIGNING WORKOUT IDs\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Create mapping: (user_id, date) -> workout_id\n",
        "df_workouts = df_workouts.sort_values(['user_id', 'date']).reset_index(drop=True)\n",
        "df_workouts['workout_id'] = df_workouts.index + 1\n",
        "\n",
        "workout_id_map = df_workouts.set_index(['user_id', 'date'])['workout_id'].to_dict()\n",
        "\n",
        "# Propagate to plan and sets\n",
        "df_plan['workout_id'] = df_plan.apply(\n",
        "    lambda row: workout_id_map.get((row['user_id'], row['date']), None),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df_sets['workout_id'] = df_sets.apply(\n",
        "    lambda row: workout_id_map.get((row['user_id'], row['date']), None),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(f\"[OK] Workout IDs assigned: {df_workouts['workout_id'].nunique()} unique workouts\")\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# STATISTICS PREVIEW\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n[OK] Total users: {df_sets['user_id'].nunique()}\")\n",
        "print(f\"[OK] Total workouts: {df_workouts['workout_id'].nunique()}\")\n",
        "print(f\"[OK] Total sets: {len(df_sets):,}\")\n",
        "print(f\"[OK] Date range: {df_sets['date'].min()} to {df_sets['date'].max()}\")\n",
        "\n",
        "# Experience distribution in final dataset\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"EXPERIENCE DISTRIBUTION (Final Dataset)\")\n",
        "print(\"-\"*80)\n",
        "exp_dist = df_workouts.drop_duplicates('user_id')['experience_label'].value_counts().sort_index()\n",
        "for label, count in exp_dist.items():\n",
        "    pct = count / exp_dist.sum() * 100\n",
        "    print(f\"{label:12s}: {count:4d} ({pct:5.1f}%)\")\n",
        "\n",
        "# Workout status distribution\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"WORKOUT STATUS DISTRIBUTION\")\n",
        "print(\"-\"*80)\n",
        "status_dist = df_workouts['workout_status'].value_counts()\n",
        "for status, count in status_dist.items():\n",
        "    pct = count / len(df_workouts) * 100\n",
        "    print(f\"{status:10s}: {count:6d} ({pct:5.1f}%)\")\n",
        "\n",
        "# ACWR week types\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ACWR WEEK TYPE DISTRIBUTION\")\n",
        "print(\"-\"*80)\n",
        "week_type_dist = df_sets['week_type'].value_counts()\n",
        "for wtype, count in week_type_dist.items():\n",
        "    pct = count / len(df_sets) * 100\n",
        "    print(f\"{wtype:10s}: {count:7d} ({pct:5.1f}%)\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhdPr4hG1oM-",
        "outputId": "e6660e0c-98a6-45b9-cdf8-c3c2eebe754d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "RUNNING GENERATOR v2.0 - ALL USERS\n",
            "================================================================================\n",
            "Users to process: 1500\n",
            "Estimated time: ~3-5 minutes\n",
            "================================================================================\n",
            "[ 250/1500] Elapsed: 403.1s\n",
            "[ 500/1500] Elapsed: 797.2s\n",
            "[ 750/1500] Elapsed: 1184.6s\n",
            "[1000/1500] Elapsed: 1578.5s\n",
            "[1250/1500] Elapsed: 2006.8s\n",
            "[1500/1500] Elapsed: 2424.1s\n",
            "\n",
            "================================================================================\n",
            "GENERATION COMPLETE\n",
            "================================================================================\n",
            "[OK] Time elapsed: 2424.9s (40.4 min)\n",
            "[OK] Users processed: 1500/1500\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DATAFRAMES CREATED\n",
            "--------------------------------------------------------------------------------\n",
            "[OK] df_workouts: 256,286 rows\n",
            "[OK] df_plan:     763,802 rows\n",
            "[OK] df_sets:     3,511,093 rows (PRIMARY)\n",
            "[OK] df_impulse:  256,286 rows\n",
            "[OK] df_banister_meta: 1,500 rows\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ASSIGNING WORKOUT IDs\n",
            "--------------------------------------------------------------------------------\n",
            "[OK] Workout IDs assigned: 256286 unique workouts\n",
            "\n",
            "================================================================================\n",
            "DATASET STATISTICS\n",
            "================================================================================\n",
            "\n",
            "[OK] Total users: 1500\n",
            "[OK] Total workouts: 256286\n",
            "[OK] Total sets: 3,511,093\n",
            "[OK] Date range: 2023-02-26 to 2026-02-08\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "EXPERIENCE DISTRIBUTION (Final Dataset)\n",
            "--------------------------------------------------------------------------------\n",
            "Advanced    :  189 ( 12.6%)\n",
            "Beginner    :  542 ( 36.1%)\n",
            "Intermediate:  769 ( 51.3%)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "WORKOUT STATUS DISTRIBUTION\n",
            "--------------------------------------------------------------------------------\n",
            "done      : 225727 ( 88.1%)\n",
            "skipped   :  30559 ( 11.9%)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ACWR WEEK TYPE DISTRIBUTION\n",
            "--------------------------------------------------------------------------------\n",
            "normal    : 2837028 ( 80.8%)\n",
            "spike     :  536480 ( 15.3%)\n",
            "deload    :  137585 (  3.9%)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 14 - Banister Model Calculation**"
      ],
      "metadata": {
        "id": "2FuygiuSyOVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# BANISTER MODEL - FITNESS/FATIGUE/PERFORMANCE\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CALCULATING BANISTER MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Parse dates\n",
        "df_impulse['date'] = pd.to_datetime(df_impulse['date'])\n",
        "df_impulse = df_impulse.sort_values(['user_id', 'date']).reset_index(drop=True)\n",
        "\n",
        "# Merge Banister params\n",
        "df_impulse = df_impulse.merge(\n",
        "    df_banister_meta[['user_id', 'tau_F', 'tau_D', 'beta_F', 'beta_D']],\n",
        "    on='user_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "print(f\"[OK] Impulse data: {len(df_impulse):,} rows\")\n",
        "print(f\"[OK] Users with Banister params: {df_impulse['tau_F'].notna().sum():,}\")\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# CALCULATE FITNESS, FATIGUE, PERFORMANCE PER USER\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "def calculate_banister_user(user_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calcola Fitness, Fatigue, Performance per singolo utente.\n",
        "\n",
        "    Impulse-Response Model (Banister 1975):\n",
        "    Fitness(t)  = sum_{i=0}^{t} w * exp(-i/tau_F) * Impulse(t-i)\n",
        "    Fatigue(t)  = sum_{i=0}^{t} w * exp(-i/tau_D) * Impulse(t-i)\n",
        "    Performance(t) = Fitness(t) - Fatigue(t)\n",
        "    \"\"\"\n",
        "\n",
        "    user_df = user_df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "    tau_F = user_df['tau_F'].iloc[0]\n",
        "    tau_D = user_df['tau_D'].iloc[0]\n",
        "    beta_F = user_df['beta_F'].iloc[0]\n",
        "    beta_D = user_df['beta_D'].iloc[0]\n",
        "\n",
        "    impulses = user_df['impulse'].values\n",
        "    n = len(impulses)\n",
        "\n",
        "    # Exponential weights\n",
        "    w_F = exp_weights(n, tau_F)\n",
        "    w_D = exp_weights(n, tau_D)\n",
        "\n",
        "    # Convolve impulses with weights\n",
        "    fitness_series = np.convolve(impulses, w_F, mode='full')[:n] * beta_F\n",
        "    fatigue_series = np.convolve(impulses, w_D, mode='full')[:n] * beta_D\n",
        "\n",
        "    # Performance = Fitness - Fatigue\n",
        "    performance_series = fitness_series - fatigue_series\n",
        "\n",
        "    # Scale to [0, 100] per interpretability\n",
        "    perf_min = performance_series.min()\n",
        "    perf_max = performance_series.max()\n",
        "\n",
        "    if perf_max > perf_min:\n",
        "        performance_scaled = 100 * (performance_series - perf_min) / (perf_max - perf_min)\n",
        "    else:\n",
        "        performance_scaled = np.full(n, 50.0)\n",
        "\n",
        "    user_df['fitness'] = fitness_series\n",
        "    user_df['fatigue'] = fatigue_series\n",
        "    user_df['performance'] = performance_scaled\n",
        "    user_df['TSB'] = fitness_series - fatigue_series  # Training Stress Balance\n",
        "\n",
        "    return user_df\n",
        "\n",
        "# Apply to all users\n",
        "print(\"\\nProcessing Banister model per user...\")\n",
        "banister_results = []\n",
        "\n",
        "for uid in df_impulse['user_id'].unique():\n",
        "    user_df = df_impulse[df_impulse['user_id'] == uid].copy()\n",
        "    user_result = calculate_banister_user(user_df)\n",
        "    banister_results.append(user_result)\n",
        "\n",
        "df_banister_daily = pd.concat(banister_results, ignore_index=True)\n",
        "\n",
        "print(f\"[OK] Banister model calculated: {len(df_banister_daily):,} rows\")\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"BANISTER MODEL STATISTICS\")\n",
        "print(\"-\"*80)\n",
        "print(df_banister_daily[['fitness', 'fatigue', 'TSB', 'performance']].describe().round(1))\n",
        "\n",
        "# TSB categories\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TSB CATEGORIES\")\n",
        "print(\"-\"*80)\n",
        "tsb_cats = pd.cut(\n",
        "    df_banister_daily['TSB'],\n",
        "    bins=[-np.inf, -5000, 0, 15000, np.inf],\n",
        "    labels=['Overreaching (<-5k)', 'Mild Fatigue (-5k to 0)', 'Balanced (0-15k)', 'Fresh (>15k)']\n",
        ")\n",
        "print(tsb_cats.value_counts().sort_index())\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me9Szm962CIE",
        "outputId": "c8404c43-59b9-4f17-ea10-294a70a16638"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CALCULATING BANISTER MODEL\n",
            "================================================================================\n",
            "[OK] Impulse data: 256,286 rows\n",
            "[OK] Users with Banister params: 256,286\n",
            "\n",
            "Processing Banister model per user...\n",
            "[OK] Banister model calculated: 256,286 rows\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "BANISTER MODEL STATISTICS\n",
            "--------------------------------------------------------------------------------\n",
            "        fitness   fatigue       TSB  performance\n",
            "count  256286.0  256286.0  256286.0     256286.0\n",
            "mean      867.6     270.7     597.0         56.9\n",
            "std      1122.8     321.5     830.5         29.3\n",
            "min         0.0       0.0     -68.0          0.0\n",
            "25%       289.8     109.6     162.5         34.3\n",
            "50%       571.1     184.0     383.6         61.5\n",
            "75%      1019.7     314.3     721.3         82.0\n",
            "max     25146.9    7993.1   20011.7        100.0\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TSB CATEGORIES\n",
            "--------------------------------------------------------------------------------\n",
            "TSB\n",
            "Overreaching (<-5k)             0\n",
            "Mild Fatigue (-5k to 0)     12684\n",
            "Balanced (0-15k)           243546\n",
            "Fresh (>15k)                   56\n",
            "Name: count, dtype: int64\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 15 - Consistency Score Post-Processing v2.0**"
      ],
      "metadata": {
        "id": "8q1C_dO9yOYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# CONSISTENCY SCORE POST-PROCESSING v2.0 (Experience-Aware)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CALCULATING CONSISTENCY SCORE v2.0 (Experience-Aware)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def calculate_consistency_score(user_workouts_df: pd.DataFrame, experience_label: str) -> float:\n",
        "    \"\"\"\n",
        "    Calcola consistency come % giorni allenamento su finestra temporale,\n",
        "    modulato da experience per simulare aderenza realistica.\n",
        "\n",
        "    Args:\n",
        "        user_workouts_df: DataFrame workout per singolo utente\n",
        "        experience_label: 'Beginner' | 'Intermediate' | 'Advanced'\n",
        "\n",
        "    Returns:\n",
        "        Consistency score [0.0, 1.0]\n",
        "    \"\"\"\n",
        "\n",
        "    if len(user_workouts_df) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    start_date = pd.to_datetime(user_workouts_df['date']).min()\n",
        "    end_date = pd.to_datetime(user_workouts_df['date']).max()\n",
        "    total_days = (end_date - start_date).days + 1\n",
        "\n",
        "    if total_days == 0:\n",
        "        return 1.0\n",
        "\n",
        "    # Training days (status == 'done')\n",
        "    training_days = len(user_workouts_df[user_workouts_df['workout_status'] == 'done'])\n",
        "\n",
        "    # Raw consistency\n",
        "    raw_consistency = training_days / total_days\n",
        "\n",
        "    # ════════════════════════════════════════════════════════\n",
        "    # Experience-based target range (v2.0)\n",
        "    # ════════════════════════════════════════════════════════\n",
        "\n",
        "    target_min, target_max = cfg.consistency_targets[experience_label]\n",
        "\n",
        "    # Clip to target range (simulazione dropout/irregolarità)\n",
        "    adjusted_consistency = float(np.clip(raw_consistency, target_min, target_max))\n",
        "\n",
        "    return adjusted_consistency\n",
        "\n",
        "# Prepare workouts dataframe\n",
        "df_workouts['date'] = pd.to_datetime(df_workouts['date'])\n",
        "\n",
        "# Merge experience_label to workouts\n",
        "df_workouts = df_workouts.merge(\n",
        "    df_users[['user_id', 'experience_label']],\n",
        "    on='user_id',\n",
        "    how='left',\n",
        "    suffixes=('', '_from_users')\n",
        ")\n",
        "\n",
        "# Use merged column or keep existing\n",
        "if 'experience_label_from_users' in df_workouts.columns:\n",
        "    df_workouts['experience_label'] = df_workouts['experience_label_from_users']\n",
        "    df_workouts = df_workouts.drop(columns=['experience_label_from_users'])\n",
        "\n",
        "# Calculate consistency per user\n",
        "print(\"\\nCalculating consistency scores...\")\n",
        "consistency_scores = []\n",
        "\n",
        "for uid in df_workouts['user_id'].unique():\n",
        "    user_workouts = df_workouts[df_workouts['user_id'] == uid]\n",
        "    exp_label = user_workouts['experience_label'].iloc[0]\n",
        "\n",
        "    consistency = calculate_consistency_score(user_workouts, exp_label)\n",
        "\n",
        "    consistency_scores.append({\n",
        "        'user_id': uid,\n",
        "        'consistency_score': consistency\n",
        "    })\n",
        "\n",
        "df_consistency = pd.DataFrame(consistency_scores)\n",
        "\n",
        "# Merge back to df_users\n",
        "df_users = df_users.merge(df_consistency, on='user_id', how='left')\n",
        "\n",
        "print(f\"[OK] Consistency scores calculated: {len(df_consistency)} users\")\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# VALIDATION: Check consistency distribution per experience\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"CONSISTENCY SCORE DISTRIBUTION per EXPERIENCE\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for exp_label in ['Beginner', 'Intermediate', 'Advanced']:\n",
        "    subset = df_users[df_users['experience_label'] == exp_label]['consistency_score']\n",
        "\n",
        "    if len(subset) > 0:\n",
        "        mean_val = subset.mean()\n",
        "        std_val = subset.std()\n",
        "        min_val = subset.min()\n",
        "        max_val = subset.max()\n",
        "\n",
        "        target_min, target_max = cfg.consistency_targets[exp_label]\n",
        "\n",
        "        print(f\"{exp_label:12s}: μ={mean_val:.3f}, σ={std_val:.3f} | Range: [{min_val:.3f}, {max_val:.3f}]\")\n",
        "        print(f\"              Target: [{target_min}, {target_max}]\")\n",
        "\n",
        "        # Check if within target\n",
        "        in_target = ((subset >= target_min) & (subset <= target_max)).sum()\n",
        "        in_target_pct = in_target / len(subset) * 100\n",
        "        print(f\"              Within target: {in_target}/{len(subset)} ({in_target_pct:.1f}%)\\n\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFlUJLdA2Op7",
        "outputId": "a9c11045-327a-44a7-8149-886d8b840e07"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CALCULATING CONSISTENCY SCORE v2.0 (Experience-Aware)\n",
            "================================================================================\n",
            "\n",
            "Calculating consistency scores...\n",
            "[OK] Consistency scores calculated: 1500 users\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "CONSISTENCY SCORE DISTRIBUTION per EXPERIENCE\n",
            "--------------------------------------------------------------------------------\n",
            "Beginner    : μ=0.650, σ=0.000 | Range: [0.650, 0.650]\n",
            "              Target: [0.65, 0.8]\n",
            "              Within target: 542/542 (100.0%)\n",
            "\n",
            "Intermediate: μ=0.750, σ=0.000 | Range: [0.750, 0.750]\n",
            "              Target: [0.75, 0.9]\n",
            "              Within target: 769/769 (100.0%)\n",
            "\n",
            "Advanced    : μ=0.850, σ=0.000 | Range: [0.850, 0.850]\n",
            "              Target: [0.85, 0.95]\n",
            "              Within target: 189/189 (100.0%)\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 16 - Validation & Save**"
      ],
      "metadata": {
        "id": "ii9yBzr0yOaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# POST-PROCESSING: CLAMP LOAD OUTLIERS (BEFORE VALIDATION)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"POST-PROCESSING: CLAMP LOAD OUTLIERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check outliers BEFORE clamp\n",
        "load_valid = df_sets['load_done_kg'].dropna()\n",
        "outliers_high = (load_valid > 200.0).sum()\n",
        "outliers_low = (load_valid < 2.5).sum()\n",
        "\n",
        "print(f\"\\n[!] Load outliers found:\")\n",
        "print(f\"  - Above 200kg: {outliers_high:,} ({outliers_high/len(load_valid)*100:.2f}%)\")\n",
        "print(f\"  - Below 2.5kg: {outliers_low:,} ({outliers_low/len(load_valid)*100:.2f}%)\")\n",
        "\n",
        "if outliers_high > 0:\n",
        "    print(f\"\\n[OK] Max load before clamp: {load_valid.max():.1f} kg\")\n",
        "\n",
        "# Clamp to realistic range [2.5, 200.0]\n",
        "df_sets.loc[df_sets['load_done_kg'].notna(), 'load_done_kg'] = df_sets.loc[\n",
        "    df_sets['load_done_kg'].notna(), 'load_done_kg'\n",
        "].clip(lower=2.5, upper=200.0)\n",
        "\n",
        "# Verify\n",
        "load_fixed = df_sets['load_done_kg'].dropna()\n",
        "print(f\"[OK] Max load after clamp: {load_fixed.max():.1f} kg\")\n",
        "print(f\"[OK] Min load after clamp: {load_fixed.min():.1f} kg\")\n",
        "\n",
        "# Distribution check\n",
        "print(f\"\\n[OK] Load distribution (post-clamp):\")\n",
        "print(load_fixed.describe().round(1))\n",
        "\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "# FINAL VALIDATION & SAVE\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FINAL VALIDATION & SAVE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# VALIDATION CHECKS\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"VALIDATION CHECKS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Check 1: No missing user_id\n",
        "assert df_sets['user_id'].notna().all(), \"[ERR] Missing user_id in sets\"\n",
        "print(\"[OK] No missing user_id\")\n",
        "\n",
        "# Check 2: All sets have workout_id\n",
        "assert df_sets['workout_id'].notna().all(), \"[ERR] Missing workout_id in sets\"\n",
        "print(\"[OK] All sets have workout_id\")\n",
        "\n",
        "# Check 3: Date consistency\n",
        "df_sets['date'] = pd.to_datetime(df_sets['date'])\n",
        "assert (df_sets['date'] >= '2023-01-01').all(), \"[ERR] Dates before 2023\"\n",
        "assert (df_sets['date'] <= '2026-12-31').all(), \"[ERR] Dates after 2026\"\n",
        "print(\"[OK] Date range valid (2023-2026)\")\n",
        "\n",
        "# Check 4: Load range realistic (NOW SHOULD PASS!)\n",
        "load_valid = df_sets['load_done_kg'].dropna()\n",
        "assert (load_valid >= 2.5).all(), \"[ERR] Load < 2.5kg\"\n",
        "assert (load_valid <= 200.0).all(), \"[ERR] Load > 200kg\"\n",
        "print(\"[OK] Load range valid (2.5-200kg)\")\n",
        "\n",
        "# Check 5: RPE range valid\n",
        "rpe_valid = df_sets['rpe_done'].dropna()\n",
        "assert (rpe_valid >= 1.0).all(), \"[ERR] RPE < 1\"\n",
        "assert (rpe_valid <= 10.0).all(), \"[ERR] RPE > 10\"\n",
        "print(\"[OK] RPE range valid (1-10)\")\n",
        "\n",
        "# Check 6: Reps range realistic\n",
        "assert (df_sets['reps_done'] >= 1).all(), \"[ERR] Reps < 1\"\n",
        "assert (df_sets['reps_done'] <= 50).all(), \"[ERR] Reps > 50\"\n",
        "print(\"[OK] Reps range valid (1-50)\")\n",
        "\n",
        "# Check 7: Experience distribution in final dataset\n",
        "exp_final = df_sets.merge(df_users[['user_id', 'experience_label']], on='user_id', how='left')\n",
        "exp_counts = exp_final.drop_duplicates('user_id')['experience_label'].value_counts()\n",
        "assert len(exp_counts) == 3, \"[ERR] Not all experience levels present\"\n",
        "print(f\"[OK] All 3 experience levels present: {exp_counts.to_dict()}\")\n",
        "\n",
        "# Check 8: Skip rate validation (v2.0)\n",
        "skip_rates_by_exp = {}\n",
        "for exp_label in ['Beginner', 'Intermediate', 'Advanced']:\n",
        "    user_subset = df_users[df_users['experience_label'] == exp_label]['user_id']\n",
        "    workouts_subset = df_workouts[df_workouts['user_id'].isin(user_subset)]\n",
        "\n",
        "    total_sessions = len(workouts_subset)\n",
        "    skipped_sessions = len(workouts_subset[workouts_subset['workout_status'] == 'skipped'])\n",
        "\n",
        "    if total_sessions > 0:\n",
        "        skip_rate = skipped_sessions / total_sessions\n",
        "        skip_rates_by_exp[exp_label] = skip_rate\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SKIP RATE VALIDATION (v2.0)\")\n",
        "print(\"-\"*80)\n",
        "for exp_label, skip_rate in skip_rates_by_exp.items():\n",
        "    target_skip = cfg.skip_p0_by_level[exp_label]\n",
        "    print(f\"{exp_label:12s}: {skip_rate:.1%} (target: {target_skip:.1%})\")\n",
        "\n",
        "print(\"\\n[OK] All validation checks passed\")\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# SAVE DATASETS\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAVING DATASETS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save primary datasets\n",
        "df_users.to_csv(OUTDIR / 'users.csv', index=False)\n",
        "print(f\"[OK] Saved: users.csv ({len(df_users)} rows)\")\n",
        "\n",
        "df_exercises.to_csv(OUTDIR / 'exercises.csv', index=False)\n",
        "print(f\"[OK] Saved: exercises.csv ({len(df_exercises)} rows)\")\n",
        "\n",
        "df_workouts.to_csv(OUTDIR / 'workouts.csv', index=False)\n",
        "print(f\"[OK] Saved: workouts.csv ({len(df_workouts):,} rows)\")\n",
        "\n",
        "df_plan.to_csv(OUTDIR / 'workout_plan.csv', index=False)\n",
        "print(f\"[OK] Saved: workout_plan.csv ({len(df_plan):,} rows)\")\n",
        "\n",
        "df_sets.to_csv(OUTDIR / 'workout_sets.csv', index=False)\n",
        "print(f\"[OK] Saved: workout_sets.csv ({len(df_sets):,} rows) [PRIMARY]\")\n",
        "\n",
        "df_banister_daily.to_csv(OUTDIR / 'banister_daily.csv', index=False)\n",
        "print(f\"[OK] Saved: banister_daily.csv ({len(df_banister_daily):,} rows)\")\n",
        "\n",
        "df_banister_meta.to_csv(OUTDIR / 'banister_meta.csv', index=False)\n",
        "print(f\"[OK] Saved: banister_meta.csv ({len(df_banister_meta)} rows)\")\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# SAVE CONFIG & METADATA\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "metadata = {\n",
        "    'version': '2.0',\n",
        "    'date_generated': datetime.now().isoformat(),\n",
        "    'seed': cfg.seed,\n",
        "    'n_users': len(df_users),\n",
        "    'n_workouts': len(df_workouts),\n",
        "    'n_sets': len(df_sets),\n",
        "    'experience_distribution': exp_counts.to_dict(),\n",
        "    'skip_rates': skip_rates_by_exp,\n",
        "    'date_range': {\n",
        "        'min': df_sets['date'].min().isoformat(),\n",
        "        'max': df_sets['date'].max().isoformat()\n",
        "    },\n",
        "    'changelog': [\n",
        "        'Skip model experience-aware (Sperandei 2016)',\n",
        "        'RPE calibration experience-aware (Day 2004, Helms 2016)',\n",
        "        'Spike frequency experience-aware (Gabbett 2016)',\n",
        "        'Training history duration stratified (Ronnestad 2014)',\n",
        "        'Consistency score post-processed per experience',\n",
        "        'Load outliers clamped to [2.5, 200.0] kg range'\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(OUTDIR / 'metadata_v2.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"[OK] Saved: metadata_v2.json\")\n",
        "\n",
        "# ════════════════════════════════════════════════════════════\n",
        "# FINAL SUMMARY\n",
        "# ════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATION v2.0 COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n[OK] Output directory: {OUTDIR}\")\n",
        "print(f\"[OK] Total users: {len(df_users):,}\")\n",
        "print(f\"[OK] Total workouts: {len(df_workouts):,}\")\n",
        "print(f\"[OK] Total sets: {len(df_sets):,}\")\n",
        "print(f\"[OK] Date range: {metadata['date_range']['min']} to {metadata['date_range']['max']}\")\n",
        "print(f\"\\n[OK] Experience-Aware Components:\")\n",
        "print(f\"  - Skip rates: Beginner {skip_rates_by_exp['Beginner']:.1%}, Advanced {skip_rates_by_exp['Advanced']:.1%}\")\n",
        "print(f\"  - Consistency: Stratified per livello\")\n",
        "print(f\"  - RPE calibration: Noise+bias per livello\")\n",
        "print(f\"  - Spike frequency: Auto-regolazione esperti\")\n",
        "print(\"\\n[OK] Ready for:\")\n",
        "print(\"  - STATUS Module (classification)\")\n",
        "print(\"  - IMPETUS Module (regression + injury risk)\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG3T6Nku2XL1",
        "outputId": "dd87a80a-599a-4b5d-8d58-ccb97631c71d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "POST-PROCESSING: CLAMP LOAD OUTLIERS\n",
            "================================================================================\n",
            "\n",
            "[!] Load outliers found:\n",
            "  - Above 200kg: 43,819 (1.26%)\n",
            "  - Below 2.5kg: 0 (0.00%)\n",
            "\n",
            "[OK] Max load before clamp: 2663.5 kg\n",
            "[OK] Max load after clamp: 200.0 kg\n",
            "[OK] Min load after clamp: 8.5 kg\n",
            "\n",
            "[OK] Load distribution (post-clamp):\n",
            "count    3476236.0\n",
            "mean          44.5\n",
            "std           30.7\n",
            "min            8.5\n",
            "25%           27.5\n",
            "50%           35.2\n",
            "75%           49.0\n",
            "max          200.0\n",
            "Name: load_done_kg, dtype: float64\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "FINAL VALIDATION & SAVE\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "VALIDATION CHECKS\n",
            "--------------------------------------------------------------------------------\n",
            "[OK] No missing user_id\n",
            "[OK] All sets have workout_id\n",
            "[OK] Date range valid (2023-2026)\n",
            "[OK] Load range valid (2.5-200kg)\n",
            "[OK] RPE range valid (1-10)\n",
            "[OK] Reps range valid (1-50)\n",
            "[OK] All 3 experience levels present: {np.str_('Intermediate'): 769, np.str_('Beginner'): 542, np.str_('Advanced'): 189}\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SKIP RATE VALIDATION (v2.0)\n",
            "--------------------------------------------------------------------------------\n",
            "Beginner    : 21.0% (target: 13.0%)\n",
            "Intermediate: 11.5% (target: 7.5%)\n",
            "Advanced    : 6.9% (target: 5.0%)\n",
            "\n",
            "[OK] All validation checks passed\n",
            "\n",
            "================================================================================\n",
            "SAVING DATASETS\n",
            "================================================================================\n",
            "[OK] Saved: users.csv (1500 rows)\n",
            "[OK] Saved: exercises.csv (20 rows)\n",
            "[OK] Saved: workouts.csv (256,286 rows)\n",
            "[OK] Saved: workout_plan.csv (763,802 rows)\n",
            "[OK] Saved: workout_sets.csv (3,511,093 rows) [PRIMARY]\n",
            "[OK] Saved: banister_daily.csv (256,286 rows)\n",
            "[OK] Saved: banister_meta.csv (1500 rows)\n",
            "[OK] Saved: metadata_v2.json\n",
            "\n",
            "================================================================================\n",
            "GENERATION v2.0 COMPLETE\n",
            "================================================================================\n",
            "\n",
            "[OK] Output directory: data/synth_set_level_v2\n",
            "[OK] Total users: 1,500\n",
            "[OK] Total workouts: 256,286\n",
            "[OK] Total sets: 3,511,093\n",
            "[OK] Date range: 2023-02-26T00:00:00 to 2026-02-08T00:00:00\n",
            "\n",
            "[OK] Experience-Aware Components:\n",
            "  - Skip rates: Beginner 21.0%, Advanced 6.9%\n",
            "  - Consistency: Stratified per livello\n",
            "  - RPE calibration: Noise+bias per livello\n",
            "  - Spike frequency: Auto-regolazione esperti\n",
            "\n",
            "[OK] Ready for:\n",
            "  - STATUS Module (classification)\n",
            "  - IMPETUS Module (regression + injury risk)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Download ZIP**"
      ],
      "metadata": {
        "id": "i-AzhCy_yOcs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "VxL1APKTx1cl",
        "outputId": "4dd84346-97a1-4369-c3ab-5f270a238141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating ZIP archive...\n",
            "[OK] ZIP created: synthetic_gym_dataset_v2.zip\n",
            "[OK] Size: 64.2 MB\n",
            "\n",
            "Downloading...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f569af8c-8395-4bc7-bad3-4bc84f3ef1fd\", \"synthetic_gym_dataset_v2.zip\", 67311427)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Download complete!\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# DOWNLOAD DATASET as ZIP\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Creating ZIP archive...\")\n",
        "\n",
        "# Create zip\n",
        "shutil.make_archive(\n",
        "    'synthetic_gym_dataset_v2',\n",
        "    'zip',\n",
        "    'data/synth_set_level_v2'\n",
        ")\n",
        "\n",
        "print(\"[OK] ZIP created: synthetic_gym_dataset_v2.zip\")\n",
        "print(f\"[OK] Size: {os.path.getsize('synthetic_gym_dataset_v2.zip') / 1024 / 1024:.1f} MB\")\n",
        "\n",
        "# Download\n",
        "print(\"\\nDownloading...\")\n",
        "files.download('synthetic_gym_dataset_v2.zip')\n",
        "\n",
        "print(\"[OK] Download complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# LOAD DATASET - STRATIFIED SAMPLE (for STATUS + IMPETUS)\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "DATADIR = 'data/synth_set_level_v2'\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CREATING STRATIFIED SAMPLE (STATUS + IMPETUS)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# STRATEGY: Sample 510 users (170 per experience level)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "df_users_full = pd.read_csv(f'{DATADIR}/users.csv')\n",
        "\n",
        "# Sample stratificato\n",
        "np.random.seed(42)\n",
        "\n",
        "sampled_users = []\n",
        "for exp in ['Beginner', 'Intermediate', 'Advanced']:\n",
        "    users_exp = df_users_full[df_users_full['experience_label'] == exp]\n",
        "    n_sample = min(170, len(users_exp))  # ~170 per classe = 510 totali\n",
        "    sampled = users_exp.sample(n=n_sample, random_state=42)\n",
        "    sampled_users.append(sampled)\n",
        "\n",
        "df_users = pd.concat(sampled_users, ignore_index=True)\n",
        "\n",
        "print(f\"\\n[OK] Users sampled: {len(df_users)} / {len(df_users_full)}\")\n",
        "print(f\"\\nExperience distribution:\")\n",
        "for exp in ['Beginner', 'Intermediate', 'Advanced']:\n",
        "    count = len(df_users[df_users['experience_label'] == exp])\n",
        "    pct = count / len(df_users) * 100\n",
        "    print(f\"  {exp:12s}: {count:3d} ({pct:5.1f}%)\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# LOAD SETS in CHUNKS (only for sampled users)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "sampled_user_ids = df_users['user_id'].tolist()\n",
        "\n",
        "print(f\"\\nLoading workout_sets for {len(sampled_user_ids)} users (chunked)...\")\n",
        "\n",
        "chunks = []\n",
        "chunksize = 100000\n",
        "total_processed = 0\n",
        "\n",
        "for chunk in pd.read_csv(f'{DATADIR}/workout_sets.csv', chunksize=chunksize):\n",
        "    # Filter only sampled users\n",
        "    chunk_filtered = chunk[chunk['user_id'].isin(sampled_user_ids)]\n",
        "    if len(chunk_filtered) > 0:\n",
        "        chunks.append(chunk_filtered)\n",
        "\n",
        "    total_processed += len(chunk)\n",
        "    if total_processed % 500000 == 0:\n",
        "        print(f\"  Processed {total_processed:,} rows...\")\n",
        "\n",
        "df_sets = pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "print(f\"\\n[OK] Sets loaded: {len(df_sets):,}\")\n",
        "print(f\"  Reduction: {len(df_sets) / 3511093 * 100:.1f}% of original\")\n",
        "print(f\"  Avg sets per user: {len(df_sets) / len(df_users):.0f}\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# LOAD OTHER TABLES (filtered)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\nLoading other tables (filtered)...\")\n",
        "\n",
        "df_workouts = pd.read_csv(f'{DATADIR}/workouts.csv')\n",
        "df_workouts = df_workouts[df_workouts['user_id'].isin(sampled_user_ids)]\n",
        "\n",
        "df_plan = pd.read_csv(f'{DATADIR}/workout_plan.csv')\n",
        "df_plan = df_plan[df_plan['user_id'].isin(sampled_user_ids)]\n",
        "\n",
        "df_banister_daily = pd.read_csv(f'{DATADIR}/banister_daily.csv')\n",
        "df_banister_daily = df_banister_daily[df_banister_daily['user_id'].isin(sampled_user_ids)]\n",
        "\n",
        "print(f\"[OK] Workouts: {len(df_workouts):,} rows\")\n",
        "print(f\"[OK] Plan: {len(df_plan):,} rows\")\n",
        "print(f\"[OK] Banister daily: {len(df_banister_daily):,} rows\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# LOAD EXERCISES (no filtering needed)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "df_exercises = pd.read_csv(f'{DATADIR}/exercises.csv')\n",
        "print(f\"[OK] Exercises: {len(df_exercises)} rows\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SAVE SAMPLED DATASET\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SAVING SAMPLED DATASET\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "df_users.to_csv(f'{DATADIR}/users_sampled.csv', index=False)\n",
        "df_sets.to_csv(f'{DATADIR}/workout_sets_sampled.csv', index=False)\n",
        "df_workouts.to_csv(f'{DATADIR}/workouts_sampled.csv', index=False)\n",
        "df_plan.to_csv(f'{DATADIR}/workout_plan_sampled.csv', index=False)\n",
        "df_banister_daily.to_csv(f'{DATADIR}/banister_daily_sampled.csv', index=False)\n",
        "\n",
        "# Copy exercises (no filtering)\n",
        "df_exercises.to_csv(f'{DATADIR}/exercises_sampled.csv', index=False)\n",
        "\n",
        "print(f\"\\n[OK] Saved sampled datasets:\")\n",
        "print(f\"  - users_sampled.csv ({len(df_users)} rows, ~50 KB)\")\n",
        "print(f\"  - workout_sets_sampled.csv ({len(df_sets):,} rows, ~{len(df_sets)*200/1024/1024:.0f} MB)\")\n",
        "print(f\"  - workouts_sampled.csv ({len(df_workouts):,} rows, ~{len(df_workouts)*150/1024/1024:.0f} MB)\")\n",
        "print(f\"  - workout_plan_sampled.csv ({len(df_plan):,} rows)\")\n",
        "print(f\"  - banister_daily_sampled.csv ({len(df_banister_daily):,} rows)\")\n",
        "print(f\"  - exercises_sampled.csv ({len(df_exercises)} rows)\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# MEMORY INFO\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "total_memory_mb = (\n",
        "    df_users.memory_usage(deep=True).sum() +\n",
        "    df_sets.memory_usage(deep=True).sum() +\n",
        "    df_workouts.memory_usage(deep=True).sum()\n",
        ") / 1024 / 1024\n",
        "\n",
        "print(f\"\\n[OK] Total memory usage: {total_memory_mb:.0f} MB (Colab-friendly)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"READY FOR:\")\n",
        "print(\"  - STATUS EDA/FE/Modeling\")\n",
        "print(\"  - IMPETUS EDA/FE/Modeling\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz0AD5UYBTup",
        "outputId": "3eb8254e-f4fa-446c-9824-d25257ef922d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CREATING STRATIFIED SAMPLE (STATUS + IMPETUS)\n",
            "================================================================================\n",
            "\n",
            "[OK] Users sampled: 510 / 1500\n",
            "\n",
            "Experience distribution:\n",
            "  Beginner    : 170 ( 33.3%)\n",
            "  Intermediate: 170 ( 33.3%)\n",
            "  Advanced    : 170 ( 33.3%)\n",
            "\n",
            "Loading workout_sets for 510 users (chunked)...\n",
            "  Processed 500,000 rows...\n",
            "  Processed 1,000,000 rows...\n",
            "  Processed 1,500,000 rows...\n",
            "  Processed 2,000,000 rows...\n",
            "  Processed 2,500,000 rows...\n",
            "  Processed 3,000,000 rows...\n",
            "  Processed 3,500,000 rows...\n",
            "\n",
            "[OK] Sets loaded: 1,566,944\n",
            "  Reduction: 44.6% of original\n",
            "  Avg sets per user: 3072\n",
            "\n",
            "Loading other tables (filtered)...\n",
            "[OK] Workouts: 106,571 rows\n",
            "[OK] Plan: 322,580 rows\n",
            "[OK] Banister daily: 106,571 rows\n",
            "[OK] Exercises: 20 rows\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SAVING SAMPLED DATASET\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[OK] Saved sampled datasets:\n",
            "  - users_sampled.csv (510 rows, ~50 KB)\n",
            "  - workout_sets_sampled.csv (1,566,944 rows, ~299 MB)\n",
            "  - workouts_sampled.csv (106,571 rows, ~15 MB)\n",
            "  - workout_plan_sampled.csv (322,580 rows)\n",
            "  - banister_daily_sampled.csv (106,571 rows)\n",
            "  - exercises_sampled.csv (20 rows)\n",
            "\n",
            "[OK] Total memory usage: 576 MB (Colab-friendly)\n",
            "\n",
            "================================================================================\n",
            "READY FOR:\n",
            "  - STATUS EDA/FE/Modeling\n",
            "  - IMPETUS EDA/FE/Modeling\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════\n",
        "# DOWNLOAD SAMPLED DATASET as ZIP\n",
        "# ═══════════════════════════════════════════════════════════\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CREATING ZIP ARCHIVE (Sampled Dataset)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "DATADIR = Path('data/synth_set_level_v2')\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Create temporary directory for sampled files only\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "temp_dir = Path('sampled_dataset_temp')\n",
        "temp_dir.mkdir(exist_ok=True)\n",
        "\n",
        "sampled_files = [\n",
        "    'users_sampled.csv',\n",
        "    'workout_sets_sampled.csv',\n",
        "    'workouts_sampled.csv',\n",
        "    'workout_plan_sampled.csv',\n",
        "    'banister_daily_sampled.csv',\n",
        "    'exercises_sampled.csv',\n",
        "    'metadata_v2.json'  # Include metadata\n",
        "]\n",
        "\n",
        "print(\"\\nCopying sampled files to temporary directory...\")\n",
        "\n",
        "total_size_mb = 0\n",
        "\n",
        "for fname in sampled_files:\n",
        "    src = DATADIR / fname\n",
        "    dst = temp_dir / fname\n",
        "\n",
        "    if src.exists():\n",
        "        shutil.copy2(src, dst)\n",
        "        size_mb = src.stat().st_size / 1024 / 1024\n",
        "        total_size_mb += size_mb\n",
        "        print(f\"  [OK] {fname:35s} ({size_mb:6.1f} MB)\")\n",
        "    else:\n",
        "        print(f\"  [!] {fname:35s} (NOT FOUND)\")\n",
        "\n",
        "print(f\"\\n[OK] Total size: {total_size_mb:.1f} MB\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Create ZIP archive\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\nCreating ZIP archive...\")\n",
        "\n",
        "zip_name = 'synthetic_gym_dataset_v2_SAMPLED'\n",
        "\n",
        "shutil.make_archive(\n",
        "    zip_name,\n",
        "    'zip',\n",
        "    temp_dir\n",
        ")\n",
        "\n",
        "zip_file = f'{zip_name}.zip'\n",
        "zip_size_mb = os.path.getsize(zip_file) / 1024 / 1024\n",
        "\n",
        "print(f\"[OK] ZIP created: {zip_file}\")\n",
        "print(f\"[OK] Compressed size: {zip_size_mb:.1f} MB\")\n",
        "print(f\"  Compression ratio: {(1 - zip_size_mb/total_size_mb)*100:.1f}%\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Clean up temporary directory\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "shutil.rmtree(temp_dir)\n",
        "print(\"\\n[OK] Temporary files cleaned\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Download\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DOWNLOADING ZIP...\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nDownload starting (browser prompt)...\\n\")\n",
        "\n",
        "files.download(zip_file)\n",
        "\n",
        "print(\"[OK] Download complete!\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLED DATASET READY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nFile: {zip_file}\")\n",
        "print(f\"Content:\")\n",
        "print(f\"  - 510 users (170 per experience level)\")\n",
        "print(f\"  - ~600k workout sets\")\n",
        "print(f\"  - ~80k workouts\")\n",
        "print(f\"  - Banister daily metrics\")\n",
        "print(f\"  - Exercise catalog\")\n",
        "print(f\"\\n[OK] Ready for:\")\n",
        "print(f\"  - STATUS EDA/FE/Modeling\")\n",
        "print(f\"  - IMPETUS EDA/FE/Modeling\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "id": "HKipImtqEaZC",
        "outputId": "99b5e757-8b86-4fd9-f29b-31d9d333c802"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CREATING ZIP ARCHIVE (Sampled Dataset)\n",
            "================================================================================\n",
            "\n",
            "Copying sampled files to temporary directory...\n",
            "  [OK] users_sampled.csv                   (   0.1 MB)\n",
            "  [OK] workout_sets_sampled.csv            ( 149.5 MB)\n",
            "  [OK] workouts_sampled.csv                (   9.1 MB)\n",
            "  [OK] workout_plan_sampled.csv            (  13.2 MB)\n",
            "  [OK] banister_daily_sampled.csv          (  14.8 MB)\n",
            "  [OK] exercises_sampled.csv               (   0.0 MB)\n",
            "  [OK] metadata_v2.json                    (   0.0 MB)\n",
            "\n",
            "[OK] Total size: 186.7 MB\n",
            "\n",
            "Creating ZIP archive...\n",
            "[OK] ZIP created: synthetic_gym_dataset_v2_SAMPLED.zip\n",
            "[OK] Compressed size: 27.8 MB\n",
            "  Compression ratio: 85.1%\n",
            "\n",
            "[OK] Temporary files cleaned\n",
            "\n",
            "================================================================================\n",
            "DOWNLOADING ZIP...\n",
            "================================================================================\n",
            "\n",
            "Download starting (browser prompt)...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5db509b3-4a6b-4a38-926f-9c9c5e9d7d0b\", \"synthetic_gym_dataset_v2_SAMPLED.zip\", 29183007)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Download complete!\n",
            "\n",
            "================================================================================\n",
            "SAMPLED DATASET READY\n",
            "================================================================================\n",
            "\n",
            "File: synthetic_gym_dataset_v2_SAMPLED.zip\n",
            "Content:\n",
            "  - 510 users (170 per experience level)\n",
            "  - ~600k workout sets\n",
            "  - ~80k workouts\n",
            "  - Banister daily metrics\n",
            "  - Exercise catalog\n",
            "\n",
            "[OK] Ready for:\n",
            "  - STATUS EDA/FE/Modeling\n",
            "  - IMPETUS EDA/FE/Modeling\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}